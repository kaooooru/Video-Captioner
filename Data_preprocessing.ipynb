{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Data-preprocessing-medium.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cv8fByhFopWR"
      },
      "source": [
        "# Download datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGrGfxHo5RFk"
      },
      "source": [
        "!wget https://bashupload.com/D7AwJ/train2014.zip\n",
        "!unzip train2014.zip\n",
        "!rm train2014.zip\n",
        "!wget https://bashupload.com/BFu0A/val2014.zip\n",
        "!unzip val2014.zip\n",
        "!rm val2014.zip\n",
        "!wget http://cs.stanford.edu/people/karpathy/deepimagesent/caption_datasets.zip\n",
        "!unzip caption_datasets.zip\n",
        "!rm caption_datasets.zip\n",
        "!rm dataset_flickr30k.json\n",
        "!rm dataset_flickr8k.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOHUwNkp5V2F"
      },
      "source": [
        "!ls train2014"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVEiLfOeoiL0"
      },
      "source": [
        "#Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdkzzzNToffK"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import h5py\n",
        "import json\n",
        "import torch\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "from random import seed, choice, sample\n",
        "import cv2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Va2JtIOomk0"
      },
      "source": [
        "#Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5Z5vtSSnnLN"
      },
      "source": [
        "# Datasets:\n",
        "#   train2014 is a folder of image files for training\n",
        "#   val2014 is a folder of image files for validation\n",
        "#   dataset_coco.json is a JSON file that tells you {image -> captions}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78RzSgTmo1eL"
      },
      "source": [
        "## Data loading\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nilDcgLU5Z6B"
      },
      "source": [
        "# Load JSON file into dict\n",
        "json_path = 'dataset_coco.json'\n",
        "with open(json_path) as json_file: \n",
        "    data = json.load(json_file)\n",
        "print(data['images'][0])\n",
        "\n",
        "# Understand how each image is captioned\n",
        "# 'filename' is the image name\n",
        "# 'filepath' is the folder name\n",
        "# 'imgid' is the id of the image\n",
        "# 'sentences' is a list of the human captioning\n",
        "# 'tokens' is a list of words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6YB8jsa5dTR"
      },
      "source": [
        "data['images'][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z33TzZEu5f-T"
      },
      "source": [
        "data['images'][0][\"sentences\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EA-ZdyxS5iVI"
      },
      "source": [
        "type(data['images'][0]['filename'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKGEnilkqXPf"
      },
      "source": [
        "# Each image may have multiple captions\n",
        "# to reduce the bias we are introducing, \n",
        "# let's use the same number of captions per image\n",
        "captions_per_image=5\n",
        "\n",
        "# Maximum number of words in a sentence\n",
        "# If the sentence has more than max_len words, we skip it\n",
        "# If the sentence has less than max_len words, we pad it with <pad>\n",
        "max_len=50\n",
        "\n",
        "# From json object to a list of (image_path, captions) pairs \n",
        "# note: captions should be a list of word lists\n",
        "train_img_cap_pairs = []\n",
        "val_img_cap_pairs = []\n",
        "\n",
        "# It contains all distinct words\n",
        "word_set = set()\n",
        "\n",
        "for img_obj in data['images']:\n",
        "    captions = []\n",
        "    for caption in img_obj['sentences']:\n",
        "        word_set.update(caption['tokens'])\n",
        "        if len(caption['tokens']) <= max_len:\n",
        "            captions.append(caption['tokens'])\n",
        "\n",
        "    # If captions is empty, what should we do here?\n",
        "    if len(captions) == 0:\n",
        "      continue\n",
        "\n",
        "    img_path = img_obj['filepath'] + '/' + img_obj['filename']\n",
        "\n",
        "    # What if this image cannot be found?\n",
        "    if not os.path.exists(img_path): \n",
        "      continue\n",
        "\n",
        "    # Append the pair to the list\n",
        "    if img_obj['split'] == 'train':\n",
        "      train_img_cap_pairs.append([img_path,captions])\n",
        "    elif img_obj['split'] == 'val':\n",
        "      val_img_cap_pairs.append([img_path,captions])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "toB3PLhP5tuG"
      },
      "source": [
        "print(train_img_cap_pairs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qay9wsqmvYFq"
      },
      "source": [
        "## Data tranformation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpnovM8wvYfY"
      },
      "source": [
        "# HDF5: HDF5 is a unique technology suite that makes possible the management\n",
        "# of extremely large and complex data collections.\n",
        "\n",
        "# 1. We will create 2 hdf5 files: \n",
        "#      train_images.hdf5, val_images.hdf5\n",
        "# 2. We will create 5 json files: \n",
        "#      word_map.json -- contains a (word -> number) hash object\n",
        "#      train_captions.json -- contains a list of encoded training captions\n",
        "#      val_captions.json -- contains a list of encoded validation captions\n",
        "#      train_caption_length.json -- contains a list of training caption lengths\n",
        "#      val_caption_length.json -- contains a list of validation caption lengths"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lU4bEZ-Ww8Bu"
      },
      "source": [
        "# Word Encoding\n",
        "# word_map: word    -> number (starting from 1)\n",
        "#           <pad>   -> 0Â·\n",
        "#           <start> -> the second highest number\n",
        "#           <end>   -> the highest number\n",
        "word_map = {k:idk+1 for idk, k in enumerate(word_set)}\n",
        "word_map['<start>'] = len(word_map) + 1\n",
        "word_map['<end>'] = len(word_map) + 1\n",
        "word_map['<pad>'] = 0\n",
        "\n",
        "# Save word map to a JSON\n",
        "with open(os.path.join('word_map.json'), 'w') as j:\n",
        "  json.dump(word_map, j)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MgtQwvvTvyWO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "7997c956-f0a1-4087-a3d7-61d17fee88b1"
      },
      "source": [
        "for img_cap_pairs, split in [[train_img_cap_pairs,'train'], [val_img_cap_pairs, 'val']]:\n",
        "    # Save encoded captions and their lengths to JSON files\n",
        "    h5py_path = os.path.join(split + '_images.hdf5')\n",
        "    \n",
        "    # remove it if the path exists\n",
        "    if os.path.exists(h5py_path): \n",
        "      os.remove(h5py_path)\n",
        "\n",
        "    with h5py.File(h5py_path, 'a') as h:\n",
        "        # Make a note of the number of captions we are sampling per image\n",
        "        h.attrs['captions_per_image'] = captions_per_image\n",
        "\n",
        "        # Create dataset inside HDF5 file to store images\n",
        "        # we do channel first for the image\n",
        "        images = h.create_dataset('images', (len(img_cap_pairs), 3, 256, 256), dtype='uint8')\n",
        "\n",
        "        enc_captions = []\n",
        "        caplens = []\n",
        "        for index, img_cap_pair in enumerate(img_cap_pairs):\n",
        "            img_path, captions = img_cap_pair\n",
        "\n",
        "            if len(captions) < captions_per_image:\n",
        "                # add some captions by randomly sampling from captions\n",
        "                captions = captions + sample(captions, captions_per_image-len(captions))\n",
        "            else:\n",
        "                # randomly sample k from captions\n",
        "                captions = sample(captions, captions_per_image)\n",
        "\n",
        "            # Sanity check\n",
        "            assert len(captions) == captions_per_image\n",
        "\n",
        "            # Read image and transform it into (3, 256, 256)\n",
        "            # Hint: use cv2, you will need to read, resize and transpose\n",
        "            img = cv2.imread(img_path, 1)\n",
        "            img = cv2.resize(img, (256,256))\n",
        "            img = img.transpose(2,0,1)\n",
        "\n",
        "            assert img.shape == (3, 256, 256)\n",
        "\n",
        "            # Save image to HDF5 file\n",
        "            images[index] = img\n",
        "\n",
        "            for idx, caption in enumerate(captions):\n",
        "                # Encode captions\n",
        "                #   a list of numbers\n",
        "                #   Format should be <start> word1 word2 ... wordN <end> <pad> <pad>...\n",
        "                #   The total length should be equal to max_len\n",
        "                enc_c = [word_map['<start>']] + [word_map[word] for word in caption] + \\\n",
        "                        [word_map['<end>']] + [word_map['<pad>']]*(max_len - len(caption))\n",
        "                enc_captions.append(enc_c)\n",
        "                caplens.append(len(caption) + 2)                \n",
        "\n",
        "    with open(os.path.join(split + '_captions.json'), 'w') as j:\n",
        "        json.dump(enc_captions, j)\n",
        "\n",
        "    with open(os.path.join(split + '_caption_length.json'), 'w') as j:\n",
        "        json.dump(caplens, j)\n",
        "\n",
        "# Sanity check\n",
        "print('caption length:', caplens[-1])\n",
        "print('caption:', caption)\n",
        "print('caption encoding:', enc_c)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "caption length: 12\n",
            "caption: ['a', 'horse', 'stands', 'on', 'grass', 'and', 'looks', 'at', 'the', 'camera']\n",
            "caption encoding: [27930, 25913, 12943, 22226, 8382, 16078, 23802, 15480, 11368, 1448, 19596, 27931, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szQPfPVnoS7N"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}