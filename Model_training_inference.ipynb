{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Inference",
      "provenance": [],
      "collapsed_sections": [
        "MK5ZR5XE5DhL",
        "9Va2JtIOomk0",
        "NPu-GjhgqnkY",
        "loGag7jyHM-1",
        "mjef7jONHOX6",
        "Gddi9-OWLkqT",
        "SCdA_a6PJBE2",
        "YcsBgmB6H6C6",
        "eWgvfAFLH_ov",
        "sMz3PcBUIGQ6"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "aa88c43e5d904fca8c0f2f3e28eab288": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_28968311a417462b8979f8c8ea457928",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_be6b2b50180b422ab541eadf3ad3d79b",
              "IPY_MODEL_61a539cd9616496aa694801bd910d21f"
            ]
          }
        },
        "28968311a417462b8979f8c8ea457928": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "be6b2b50180b422ab541eadf3ad3d79b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_53bfe85c64fb4f79ae5f024f7898d2bf",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 178728960,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 178728960,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_31120c22786e4d1b82d414c75c1ac290"
          }
        },
        "61a539cd9616496aa694801bd910d21f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_22c5a0bd033041fb99ff967353d3bfcf",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170M/170M [00:08&lt;00:00, 22.2MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6bcc4137e51f47549d0a371bd0ec9ee5"
          }
        },
        "53bfe85c64fb4f79ae5f024f7898d2bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "31120c22786e4d1b82d414c75c1ac290": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "22c5a0bd033041fb99ff967353d3bfcf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6bcc4137e51f47549d0a371bd0ec9ee5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MK5ZR5XE5DhL"
      },
      "source": [
        "# Download dataset *"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ecui9Kavbb74"
      },
      "source": [
        "!wget https://bashupload.com/-nc5I/train2014.zip\n",
        "!unzip train2014.zip\n",
        "!rm train2014.zip\n",
        "!wget https://bashupload.com/baIo3/val2014.zip\n",
        "!unzip val2014.zip\n",
        "!rm val2014.zip\n",
        "!wget http://cs.stanford.edu/people/karpathy/deepimagesent/caption_datasets.zip\n",
        "!unzip caption_datasets.zip\n",
        "!rm caption_datasets.zip\n",
        "!rm dataset_flickr30k.json\n",
        "!rm dataset_flickr8k.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6R_uFJIAJrR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9835f44f-6613-4887-cc6d-45353cd0accf"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhhvLNhgbY0X"
      },
      "source": [
        "!cp /content/drive/My\\ Drive/bootcamp/ML\\ fundamentals/image\\ captioning/train2014.zip ./\n",
        "!unzip train2014.zip\n",
        "!rm train2014.zip\n",
        "!cp /content/drive/My\\ Drive/bootcamp/ML\\ fundamentals/image\\ captioning/val2014.zip ./\n",
        "!unzip val2014.zip\n",
        "!rm val2014.zip\n",
        "!wget http://cs.stanford.edu/people/karpathy/deepimagesent/caption_datasets.zip\n",
        "!unzip caption_datasets.zip\n",
        "!rm caption_datasets.zip\n",
        "!rm dataset_flickr30k.json\n",
        "!rm dataset_flickr8k.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Va2JtIOomk0"
      },
      "source": [
        "#Data preprocessing *"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92LVl2M6q-NH"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import h5py\n",
        "import json\n",
        "import torch\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "from random import seed, choice, sample\n",
        "import cv2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5Z5vtSSnnLN"
      },
      "source": [
        "# Datasets:\n",
        "#   train2014 is a folder of image files for training\n",
        "#   val2014 is a folder of image files for validation\n",
        "#   dataset_coco.json is a JSON file that tells {image -> captions}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78RzSgTmo1eL"
      },
      "source": [
        "## Data loading *\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZwfFwbgo5qW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "f6e33259-4a73-464c-c1e1-7215ed49fbd9"
      },
      "source": [
        "# Load JSON file into dict\n",
        "json_path = 'dataset_coco.json'\n",
        "with open(json_path, 'r') as j:\n",
        "    data = json.load(j)\n",
        "print(data['images'][0])\n",
        "\n",
        "# 'filename' is the image name\n",
        "# 'filepath' is the folder name\n",
        "# 'imgid' is the id of the image\n",
        "# 'sentences' is a list of the human captioning\n",
        "# 'tokens' is a list of words"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'filepath': 'val2014', 'sentids': [770337, 771687, 772707, 776154, 781998], 'filename': 'COCO_val2014_000000391895.jpg', 'imgid': 0, 'split': 'test', 'sentences': [{'tokens': ['a', 'man', 'with', 'a', 'red', 'helmet', 'on', 'a', 'small', 'moped', 'on', 'a', 'dirt', 'road'], 'raw': 'A man with a red helmet on a small moped on a dirt road. ', 'imgid': 0, 'sentid': 770337}, {'tokens': ['man', 'riding', 'a', 'motor', 'bike', 'on', 'a', 'dirt', 'road', 'on', 'the', 'countryside'], 'raw': 'Man riding a motor bike on a dirt road on the countryside.', 'imgid': 0, 'sentid': 771687}, {'tokens': ['a', 'man', 'riding', 'on', 'the', 'back', 'of', 'a', 'motorcycle'], 'raw': 'A man riding on the back of a motorcycle.', 'imgid': 0, 'sentid': 772707}, {'tokens': ['a', 'dirt', 'path', 'with', 'a', 'young', 'person', 'on', 'a', 'motor', 'bike', 'rests', 'to', 'the', 'foreground', 'of', 'a', 'verdant', 'area', 'with', 'a', 'bridge', 'and', 'a', 'background', 'of', 'cloud', 'wreathed', 'mountains'], 'raw': 'A dirt path with a young person on a motor bike rests to the foreground of a verdant area with a bridge and a background of cloud-wreathed mountains. ', 'imgid': 0, 'sentid': 776154}, {'tokens': ['a', 'man', 'in', 'a', 'red', 'shirt', 'and', 'a', 'red', 'hat', 'is', 'on', 'a', 'motorcycle', 'on', 'a', 'hill', 'side'], 'raw': 'A man in a red shirt and a red hat is on a motorcycle on a hill side.', 'imgid': 0, 'sentid': 781998}], 'cocoid': 391895}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkqMs27ybTrl"
      },
      "source": [
        "data[\"images\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKGEnilkqXPf"
      },
      "source": [
        "# Each image may have multiple captions\n",
        "# to reduce the bias we are introducing, \n",
        "# use the same number of captions per image\n",
        "captions_per_image=5\n",
        "\n",
        "# Maximum number of words in a sentence\n",
        "# If the sentence has more than max_len words, skip it\n",
        "# If the sentence has less than max_len words, pad it with <pad>\n",
        "max_len=50\n",
        "\n",
        "# From json object to a list of (image_path, captions) pairs \n",
        "# note: captions should be a list of word lists\n",
        "train_img_cap_pairs = []\n",
        "val_img_cap_pairs = []\n",
        "test_img_cap_pairs = []\n",
        "\n",
        "# It contains all distinct words\n",
        "word_set = set()\n",
        "\n",
        "for img_obj in data['images']:\n",
        "    captions = []\n",
        "    for caption in img_obj['sentences']:\n",
        "        word_set.update(caption['tokens'])\n",
        "        if len(caption['tokens']) <= max_len:\n",
        "            captions.append(caption['tokens'])\n",
        "\n",
        "    # If captions is empty\n",
        "    if len(captions) == 0:\n",
        "        continue\n",
        "\n",
        "    img_path = os.path.join(img_obj['filepath'], img_obj['filename'])\n",
        "\n",
        "    # If image cannot be found\n",
        "    if not os.path.exists(img_path): continue\n",
        "\n",
        "    # Append the pair to the list\n",
        "    if img_obj['split'] == 'train':\n",
        "      train_img_cap_pairs.append([img_path, captions])\n",
        "    elif img_obj['split'] == 'val':\n",
        "      val_img_cap_pairs.append([img_path, captions])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qay9wsqmvYFq"
      },
      "source": [
        "## Data tranformation *"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpnovM8wvYfY"
      },
      "source": [
        "# HDF5: HDF5 is a unique technology suite that makes possible the management\n",
        "# of extremely large and complex data collections.\n",
        "\n",
        "# 1. Will create 2 hdf5 files: \n",
        "#      train_images.hdf5, val_images.hdf5\n",
        "# 2. Will create 5 json files: \n",
        "#      word_map.json -- contains a (word -> number) hash object\n",
        "#      train_captions.json -- contains a list of encoded training captions\n",
        "#      val_captions.json -- contains a list of encoded validation captions\n",
        "#      train_caption_length.json -- contains a list of training caption lengths\n",
        "#      val_caption_length.json -- contains a list of validation caption lengths"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lU4bEZ-Ww8Bu"
      },
      "source": [
        "# Word Encoding\n",
        "# word_map: word    -> number (starting from 1)\n",
        "#           <pad>   -> 0\n",
        "#           <start> -> the second highest number\n",
        "#           <end>   -> the highest number\n",
        "word_map = {k: idx + 1 for idx, k in enumerate(word_set)}\n",
        "word_map['<start>'] = len(word_map) + 1\n",
        "word_map['<end>'] = len(word_map) + 1\n",
        "word_map['<pad>'] = 0\n",
        "\n",
        "# Save word map to a JSON\n",
        "with open(os.path.join('word_map.json'), 'w') as j:\n",
        "  json.dump(word_map, j)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MgtQwvvTvyWO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "f81f871a-07d6-43e7-a792-13f410e7943b"
      },
      "source": [
        "\n",
        "for img_cap_pairs, split in [[train_img_cap_pairs,'train'], [train_img_cap_pairs, 'val']]:\n",
        "    # Save encoded captions and their lengths to JSON files\n",
        "    h5py_path = os.path.join(split + '_images.hdf5')\n",
        "    \n",
        "    # remove it if the path exists\n",
        "    if os.path.exists(h5py_path): os.remove(h5py_path)\n",
        "\n",
        "    with h5py.File(h5py_path, 'a') as h:\n",
        "        # Make a note of the number of captions we are sampling per image\n",
        "        h.attrs['captions_per_image'] = captions_per_image\n",
        "\n",
        "        # Create dataset inside HDF5 file to store images\n",
        "        # do channel first for the image\n",
        "        images = h.create_dataset('images', (len(img_cap_pairs), 3, 256, 256), dtype='uint8')\n",
        "\n",
        "        enc_captions = []\n",
        "        caplens = []\n",
        "        for index, img_cap_pair in enumerate(img_cap_pairs):\n",
        "            img_path, captions = img_cap_pair\n",
        "\n",
        "            if len(captions) < captions_per_image:\n",
        "                # add some captions by randomly sampling from captions\n",
        "                captions = captions + [choice(captions) for _ in range(captions_per_image - len(captions))]\n",
        "            else:\n",
        "                # randomly sample k from captions\n",
        "                captions = sample(captions, captions_per_image)\n",
        "\n",
        "            # Sanity check\n",
        "            assert len(captions) == captions_per_image\n",
        "\n",
        "            # Read image and transform it into (3, 256, 256)\n",
        "            # use cv2, need to read, resize and transpose\n",
        "            img = cv2.imread(img_path)\n",
        "            img = cv2.resize(img, (256, 256))\n",
        "            img = img.transpose(2, 0, 1)\n",
        "\n",
        "            assert img.shape == (3, 256, 256)\n",
        "\n",
        "            # Save image to HDF5 file\n",
        "            images[index] = img\n",
        "            for idx, caption in enumerate(captions):\n",
        "                # Encode captions\n",
        "                #   a list of numbers\n",
        "                #   Format should be <start> word1 word2 ... wordN <end> <pad> <pad>...\n",
        "                #   The total length should be equal to max_len\n",
        "                enc_c = [word_map['<start>']] + [word_map[word] for word in caption] + \\\n",
        "                 [word_map['<end>']] + [word_map['<pad>']] * (max_len - len(caption))\n",
        "\n",
        "                enc_captions.append(enc_c)\n",
        "                caplens.append(len(caption) + 2)                \n",
        "\n",
        "    with open(os.path.join(split + '_captions.json'), 'w') as j:\n",
        "        json.dump(enc_captions, j)\n",
        "\n",
        "    with open(os.path.join(split + '_caption_length.json'), 'w') as j:\n",
        "        json.dump(caplens, j)\n",
        "\n",
        "# Sanity check\n",
        "print('caption length:', caplens[0])\n",
        "print('caption:', caption)\n",
        "print('caption encoding:', enc_c)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "caption length: 13\n",
            "caption: ['a', 'plate', 'of', 'crinkle', 'fries', 'and', 'panini', 'sandwiches']\n",
            "caption encoding: [27930, 6413, 26063, 8787, 2378, 26541, 27042, 3220, 6744, 27931, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPu-GjhgqnkY"
      },
      "source": [
        "#Helper functions *"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Br2WmVaQl8zd"
      },
      "source": [
        "def save_checkpoint(data_name, epoch, epochs_since_improvement, encoder, decoder, encoder_optimizer, decoder_optimizer,\n",
        "                    bleu4, is_best):\n",
        "    state = {'epoch': epoch,\n",
        "             'epochs_since_improvement': epochs_since_improvement,\n",
        "             'bleu-4': bleu4,\n",
        "             'encoder': encoder,\n",
        "             'decoder': decoder,\n",
        "             'encoder_optimizer': encoder_optimizer,\n",
        "             'decoder_optimizer': decoder_optimizer}\n",
        "    filename = 'checkpoint_' + data_name + '.pth.tar'\n",
        "    torch.save(state, filename)\n",
        "    # If this checkpoint is the best so far, store a copy so it doesn't get overwritten by a worse checkpoint\n",
        "    if is_best:\n",
        "        torch.save(state, 'BEST_' + filename)\n",
        "\n",
        "\n",
        "class AverageMeter(object):\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "def accuracy(scores, targets, k):\n",
        "    batch_size = targets.size(0)\n",
        "    _, ind = scores.topk(k, 1, True, True)\n",
        "    correct = ind.eq(targets.view(-1, 1).expand_as(ind))\n",
        "    correct_total = correct.view(-1).float().sum()  # 0D tensor\n",
        "    return correct_total.item() * (100.0 / batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fnKTTtm5Ru2"
      },
      "source": [
        "# Model Architecture *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLwXhQwCHIb3"
      },
      "source": [
        "## Encoder *"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXd4HvZ0qmw2"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torchvision\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, encoded_image_size=14):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.enc_image_size = encoded_image_size\n",
        "\n",
        "        resnet = torchvision.models.resnet101(pretrained=True)  # pretrained ImageNet ResNet-101\n",
        "\n",
        "        # Remove linear and pool layers\n",
        "        modules = list(resnet.children())[:-2]\n",
        "        self.resnet = nn.Sequential(*modules)\n",
        "\n",
        "        # Resize image to fixed size to allow input images of variable size\n",
        "        self.adaptive_pool = nn.AdaptiveAvgPool2d((encoded_image_size, encoded_image_size))\n",
        "\n",
        "        # Want to avoid training encoder\n",
        "        for p in self.resnet.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "    def forward(self, images):\n",
        "        # Use resnet, apply adaptive_pool\n",
        "        out = self.resnet(images)  # (batch_size, 2048, image_size/32, image_size/32)\n",
        "        out = self.adaptive_pool(out)  # (batch_size, 2048, encoded_image_size, encoded_image_size)\n",
        "        out = out.permute(0, 2, 3, 1)  # (batch_size, encoded_image_size, encoded_image_size, 2048)\n",
        "        return out\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loGag7jyHM-1"
      },
      "source": [
        "##Attention *"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nd2Gpxu0HP2h"
      },
      "source": [
        "class Attention(nn.Module):\n",
        "    \"\"\"\n",
        "    Attention Network.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n",
        "        \"\"\"\n",
        "        :param encoder_dim: feature size of encoded images\n",
        "        :param decoder_dim: size of decoder's RNN\n",
        "        :param attention_dim: size of the attention network\n",
        "        \"\"\"\n",
        "        super(Attention, self).__init__()\n",
        "        self.encoder_att = nn.Linear(encoder_dim, attention_dim)  # linear layer to transform encoded image\n",
        "        self.decoder_att = nn.Linear(decoder_dim, attention_dim)  # linear layer to transform decoder's output\n",
        "        self.full_att = nn.Linear(attention_dim, 1)  # linear layer to calculate values to be softmax-ed\n",
        "        self.relu = nn.ReLU()\n",
        "        self.softmax = nn.Softmax(dim=1)  # softmax layer to calculate weights\n",
        "\n",
        "    def forward(self, encoder_out, decoder_hidden):\n",
        "        \"\"\"\n",
        "        Forward propagation.\n",
        "        :param encoder_out: encoded images, a tensor of dimension (batch_size, num_pixels, encoder_dim)\n",
        "        :param decoder_hidden: previous decoder output, a tensor of dimension (batch_size, decoder_dim)\n",
        "        :return: attention weighted encoding, weights\n",
        "        \"\"\"\n",
        "        att1 = self.encoder_att(encoder_out)  # (batch_size, num_pixels, attention_dim)\n",
        "        att2 = self.decoder_att(decoder_hidden)  # (batch_size, attention_dim)\n",
        "        att = self.full_att(self.relu(att1 + att2.unsqueeze(1))).squeeze(2)  # (batch_size, num_pixels)\n",
        "        alpha = self.softmax(att)  # (batch_size, num_pixels)\n",
        "        attention_weighted_encoding = (encoder_out * alpha.unsqueeze(2)).sum(dim=1)  # (batch_size, encoder_dim)\n",
        "\n",
        "        return attention_weighted_encoding, alpha\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjef7jONHOX6"
      },
      "source": [
        "##Decoder *"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_N6-RqyLHUvt"
      },
      "source": [
        "\n",
        "class DecoderWithAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Decoder.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, attention_dim, embed_dim, decoder_dim, vocab_size, encoder_dim=2048, dropout=0.5):\n",
        "        \"\"\"\n",
        "        :param attention_dim: size of attention network\n",
        "        :param embed_dim: embedding size\n",
        "        :param decoder_dim: size of decoder's RNN\n",
        "        :param vocab_size: size of vocabulary\n",
        "        :param encoder_dim: feature size of encoded images\n",
        "        :param dropout: dropout\n",
        "        \"\"\"\n",
        "        super(DecoderWithAttention, self).__init__()\n",
        "\n",
        "        self.encoder_dim = encoder_dim\n",
        "        self.attention_dim = attention_dim\n",
        "        self.embed_dim = embed_dim\n",
        "        self.decoder_dim = decoder_dim\n",
        "        self.vocab_size = vocab_size\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.attention = Attention(encoder_dim, decoder_dim, attention_dim)  # attention network\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)  # embedding layer\n",
        "        self.dropout = nn.Dropout(p=self.dropout)\n",
        "        self.decode_step = nn.LSTMCell(embed_dim + encoder_dim, decoder_dim, bias=True)  # decoding LSTMCell\n",
        "        self.init_h = nn.Linear(encoder_dim, decoder_dim)  # linear layer to find initial hidden state of LSTMCell\n",
        "        self.init_c = nn.Linear(encoder_dim, decoder_dim)  # linear layer to find initial cell state of LSTMCell\n",
        "        self.f_beta = nn.Linear(decoder_dim, encoder_dim)  # linear layer to create a sigmoid-activated gate\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.fc = nn.Linear(decoder_dim, vocab_size)  # linear layer to find scores over vocabulary\n",
        "        self.init_weights()  # initialize some layers with the uniform distribution\n",
        "\n",
        "    def init_weights(self):\n",
        "        \"\"\"\n",
        "        Initializes some parameters with values from the uniform distribution, for easier convergence.\n",
        "        \"\"\"\n",
        "        self.embedding.weight.data.uniform_(-0.1, 0.1)\n",
        "        self.fc.bias.data.fill_(0)\n",
        "        self.fc.weight.data.uniform_(-0.1, 0.1)\n",
        "\n",
        "    def init_hidden_state(self, encoder_out):\n",
        "        \"\"\"\n",
        "        Creates the initial hidden and cell states for the decoder's LSTM based on the encoded images.\n",
        "        :param encoder_out: encoded images, a tensor of dimension (batch_size, num_pixels, encoder_dim)\n",
        "        :return: hidden state, cell state\n",
        "        \"\"\"\n",
        "        mean_encoder_out = encoder_out.mean(dim=1)\n",
        "        h = self.init_h(mean_encoder_out)  # (batch_size, decoder_dim)\n",
        "        c = self.init_c(mean_encoder_out)\n",
        "        return h, c\n",
        "\n",
        "    def forward(self, encoder_out, encoded_captions, caption_lengths):\n",
        "        \"\"\"\n",
        "        Forward propagation.\n",
        "        :param encoder_out: encoded images, a tensor of dimension (batch_size, enc_image_size, enc_image_size, encoder_dim)\n",
        "        :param encoded_captions: encoded captions, a tensor of dimension (batch_size, max_caption_length)\n",
        "        :param caption_lengths: caption lengths, a tensor of dimension (batch_size, 1)\n",
        "        :return: scores for vocabulary, sorted encoded captions, decode lengths, weights, sort indices\n",
        "        \"\"\"\n",
        "\n",
        "        batch_size = encoder_out.size(0)\n",
        "        encoder_dim = encoder_out.size(-1)\n",
        "        vocab_size = self.vocab_size\n",
        "\n",
        "        # Flatten image\n",
        "        encoder_out = encoder_out.view(batch_size, -1, encoder_dim)  # (batch_size, num_pixels, encoder_dim)\n",
        "        num_pixels = encoder_out.size(1)\n",
        "\n",
        "        # Sort input data by decreasing lengths\n",
        "        caption_lengths, sort_ind = caption_lengths.squeeze(1).sort(dim=0, descending=True)\n",
        "        encoder_out = encoder_out[sort_ind]\n",
        "        encoded_captions = encoded_captions[sort_ind]\n",
        "\n",
        "        # Embedding\n",
        "        embeddings = self.embedding(encoded_captions)  # (batch_size, max_caption_length, embed_dim)\n",
        "\n",
        "        # Initialize LSTM state\n",
        "        h, c = self.init_hidden_state(encoder_out)  # (batch_size, decoder_dim)\n",
        "\n",
        "        # Won't decode at the <end> position, since it've finished generating as soon as it generate <end>\n",
        "        # So, decoding lengths are actual lengths - 1\n",
        "        decode_lengths = (caption_lengths - 1).tolist()\n",
        "\n",
        "        # Create tensors to hold word predicion scores and alphas\n",
        "        predictions = torch.zeros(batch_size, max(decode_lengths), vocab_size).to(device)\n",
        "        alphas = torch.zeros(batch_size, max(decode_lengths), num_pixels).to(device)\n",
        "\n",
        "        # At each time-step, decode by\n",
        "        # attention-weighing the encoder's output based on the decoder's previous hidden state output\n",
        "        # then generate a new word in the decoder with the previous word and the attention weighted encoding\n",
        "        for t in range(max(decode_lengths)):\n",
        "            batch_size_t = sum([l > t for l in decode_lengths])\n",
        "            attention_weighted_encoding, alpha = self.attention(encoder_out[:batch_size_t],\n",
        "                                                                h[:batch_size_t])\n",
        "            gate = self.sigmoid(self.f_beta(h[:batch_size_t]))  # gating scalar, (batch_size_t, encoder_dim)\n",
        "            attention_weighted_encoding = gate * attention_weighted_encoding\n",
        "            h, c = self.decode_step(\n",
        "                torch.cat([embeddings[:batch_size_t, t, :], attention_weighted_encoding], dim=1),\n",
        "                (h[:batch_size_t], c[:batch_size_t]))  # (batch_size_t, decoder_dim)\n",
        "            preds = self.fc(self.dropout(h))  # (batch_size_t, vocab_size)\n",
        "            predictions[:batch_size_t, t, :] = preds\n",
        "            alphas[:batch_size_t, t, :] = alpha\n",
        "\n",
        "        return predictions, encoded_captions, decode_lengths, alphas, sort_ind"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gddi9-OWLkqT"
      },
      "source": [
        "# Pytoch dataset transformation *\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egEdtSr267C7"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import h5py\n",
        "import json\n",
        "import os\n",
        "\n",
        "\n",
        "class CaptionDataset(Dataset):\n",
        "    def __init__(self, split):\n",
        "        \"\"\"\n",
        "        :param split: split, one of 'TRAIN', 'VAL', or 'TEST'\n",
        "        \"\"\"\n",
        "        self.split = split\n",
        "\n",
        "        # Open hdf5 file where images are stored\n",
        "        self.h = h5py.File(os.path.join(self.split + '_images.hdf5'), 'r')\n",
        "        self.imgs = self.h['images']\n",
        "\n",
        "        # Captions per image\n",
        "        self.cpi = self.h.attrs['captions_per_image']\n",
        "\n",
        "        # Load encoded captions \n",
        "        with open(os.path.join(self.split + '_captions.json'), 'r') as j:\n",
        "            self.captions = json.load(j)\n",
        "\n",
        "        # Load caption lengths\n",
        "        with open(os.path.join(self.split + '_caption_length.json'), 'r') as j:\n",
        "            self.caplens = json.load(j)\n",
        "\n",
        "        # Total number of datapoints\n",
        "        self.dataset_size = len(self.captions)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        # The Nth caption corresponds to the (N // captions_per_image)th image\n",
        "        img = torch.FloatTensor(self.imgs[i // self.cpi] / 255.)\n",
        "        normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        img = transforms.Compose([normalize])(img)\n",
        "\n",
        "        caption = torch.LongTensor(self.captions[i])\n",
        "        caplen = torch.LongTensor([self.caplens[i]])\n",
        "\n",
        "        if self.split is 'train':\n",
        "            return img, caption, caplen\n",
        "        else:\n",
        "            # For validation of testing, also return all 'captions_per_image' captions to find BLEU-4 score\n",
        "            all_captions = torch.LongTensor(\n",
        "                self.captions[((i // self.cpi) * self.cpi):(((i // self.cpi) * self.cpi) + self.cpi)])\n",
        "            return img, caption, caplen, all_captions\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.dataset_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fh54sUaU5aUk"
      },
      "source": [
        "# Model training *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCdA_a6PJBE2"
      },
      "source": [
        "##Initialize parameters *"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZTWV83qq6dy"
      },
      "source": [
        "import time\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim\n",
        "import torch.utils.data\n",
        "import torchvision.transforms as transforms\n",
        "from torch import nn\n",
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "# Model hyper-parameters\n",
        "emb_dim = 512  # dimension of word embeddings\n",
        "attention_dim = 512  # dimension of attention linear layers\n",
        "decoder_dim = 512  # dimension of decoder RNN\n",
        "dropout = 0.5\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  \n",
        "cudnn.benchmark = True  \n",
        "\n",
        "epochs = 30  \n",
        "batch_size = 32\n",
        "decoder_lr = 4e-4  \n",
        "alpha_c = 1.  # regularization parameter for 'doubly stochastic attention'\n",
        "best_bleu4 = 0.  # BLEU-4 score right now"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YcsBgmB6H6C6"
      },
      "source": [
        "## Training per epoch *"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRS2xtHsH5nu"
      },
      "source": [
        "def train(train_loader, encoder, decoder, loss_function, decoder_optimizer, epoch):\n",
        "    \"\"\"\n",
        "    Performs one epoch's training.\n",
        "    :param train_loader: DataLoader for training data\n",
        "    :param encoder: encoder model\n",
        "    :param decoder: decoder model\n",
        "    :param loss_function: loss layer\n",
        "    :param decoder_optimizer: optimizer to update decoder's weights\n",
        "    :param epoch: epoch number\n",
        "    \"\"\"\n",
        "\n",
        "    decoder.train()  # train mode (dropout and batchnorm is used)\n",
        "    encoder.train()\n",
        "\n",
        "    losses = AverageMeter()  # loss (per word decoded)\n",
        "    top5accs = AverageMeter()  # top5 accuracy\n",
        "\n",
        "    # Load by batches\n",
        "    for i, (imgs, caps, caplens) in enumerate(train_loader):\n",
        "        # Remember to use GPU\n",
        "        imgs = imgs.to(device)\n",
        "        caps = caps.to(device)\n",
        "        caplens = caplens.to(device)\n",
        "\n",
        "        # Encoding\n",
        "        encoded_imgs = encoder(imgs)\n",
        "        # Decoding\n",
        "        preds, caps_sorted, decode_lengths, alphas, sort_ind = decoder(encoded_imgs, caps, caplens)\n",
        "\n",
        "        # Since decoded starting with <start>, the targets are all words after <start>, up to <end>\n",
        "        targets = caps_sorted[:, 1:]\n",
        "\n",
        "        # Remove timesteps that we didn't decode at, or are pads\n",
        "        preds, _, _, _ = pack_padded_sequence(preds, decode_lengths, batch_first=True)\n",
        "        targets, _, _, _ = pack_padded_sequence(targets, decode_lengths, batch_first=True)\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = loss_function(preds, targets)\n",
        "\n",
        "        # Add doubly stochastic attention regularization\n",
        "        loss += alpha_c * ((1. - alphas.sum(dim=1)) ** 2).mean()\n",
        "\n",
        "        # Back prop on decoder only\n",
        "        loss.backward()\n",
        "      \n",
        "        # Update weights\n",
        "        decoder_optimizer.step()\n",
        "        decoder_optimizer.zero_grad()\n",
        "\n",
        "        # Keep track of metrics\n",
        "        losses.update(loss.item(), sum(decode_lengths))\n",
        "        top5accs.update(accuracy(preds, targets, 5), sum(decode_lengths))\n",
        "\n",
        "        # Print status\n",
        "        if i % 100 == 0:\n",
        "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
        "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
        "                  'Top-5 Accuracy {top5.val:.3f} ({top5.avg:.3f})'.format(epoch, i, len(train_loader),\n",
        "                                                                          loss=losses,\n",
        "                                                                          top5=top5accs))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWgvfAFLH_ov"
      },
      "source": [
        "## Validation per epoch *"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUWGxZZ3H_z_"
      },
      "source": [
        "def validate(val_loader, encoder, decoder, loss_function):\n",
        "    \"\"\"\n",
        "    Performs one epoch's validation.\n",
        "    :param val_loader: DataLoader for validation data.\n",
        "    :param encoder: encoder model\n",
        "    :param decoder: decoder model\n",
        "    :param loss_function: loss layer\n",
        "    :return: BLEU-4 score\n",
        "    \"\"\"\n",
        "    # eval mode (no dropout or batchnorm)\n",
        "    decoder.eval()  \n",
        "    encoder.eval()\n",
        "\n",
        "    losses = AverageMeter()\n",
        "    top5accs = AverageMeter()\n",
        "\n",
        "    references = list()  # references (true captions) for calculating BLEU-4 score\n",
        "    hypotheses = list()  # hypotheses (predictions)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Batches\n",
        "        for i, (imgs, caps, caplens, allcaps) in enumerate(val_loader):\n",
        "            imgs = imgs.to(device)\n",
        "            caps = caps.to(device)\n",
        "            caplens = caplens.to(device)\n",
        "\n",
        "            # Forward prop.\n",
        "            encoded_imgs = encoder(imgs)\n",
        "            scores, caps_sorted, decode_lengths, alphas, sort_ind = decoder(encoded_imgs, caps, caplens)\n",
        "            targets = caps_sorted[:, 1:]\n",
        "\n",
        "            # Remove timesteps that we didn't decode at, or are pads\n",
        "            # pack_padded_sequence is an easy trick to do this\n",
        "            scores_copy = scores.clone()\n",
        "\n",
        "            scores, _, _, _ = pack_padded_sequence(scores, decode_lengths, batch_first=True)\n",
        "            targets, _, _, _ = pack_padded_sequence(targets, decode_lengths, batch_first=True)\n",
        "\n",
        "            # Calculate loss\n",
        "            loss = loss_function(scores, targets)\n",
        "\n",
        "            # Add doubly stochastic attention regularization\n",
        "            loss += alpha_c * ((1. - alphas.sum(dim=1)) ** 2).mean()\n",
        "\n",
        "            # Keep track of metrics\n",
        "            losses.update(loss.item(), sum(decode_lengths))\n",
        "            top5 = accuracy(scores, targets, 5)\n",
        "            top5accs.update(top5, sum(decode_lengths))\n",
        "\n",
        "            if i % 100 == 0:\n",
        "                print('Validation: [{0}/{1}]\\t'\n",
        "                      'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
        "                      'Top-5 Accuracy {top5.val:.3f} ({top5.avg:.3f})\\t'.format(i, len(val_loader),\n",
        "                                                                                loss=losses, top5=top5accs))\n",
        "\n",
        "            # Store references (true captions), and hypothesis (prediction) for each image\n",
        "            # If for n images, we have n hypotheses, and references a, b, c... for each image, we need -\n",
        "            # references = [[ref1a, ref1b, ref1c], [ref2a, ref2b], ...], hypotheses = [hyp1, hyp2, ...]\n",
        "\n",
        "            # References\n",
        "            allcaps = allcaps[sort_ind]  # because images were sorted in the decoder\n",
        "            for j in range(allcaps.shape[0]):\n",
        "                img_caps = allcaps[j].tolist()\n",
        "                img_captions = list(\n",
        "                    map(lambda c: [w for w in c if w not in {word_map['<start>'], word_map['<pad>']}],\n",
        "                        img_caps))  # remove <start> and pads\n",
        "                references.append(img_captions)\n",
        "\n",
        "            # Hypotheses\n",
        "            _, preds = torch.max(scores_copy, dim=2)\n",
        "            preds = preds.tolist()\n",
        "            temp_preds = list()\n",
        "            for j, p in enumerate(preds):\n",
        "                temp_preds.append(preds[j][:decode_lengths[j]])  # remove pads\n",
        "            preds = temp_preds\n",
        "            hypotheses.extend(preds)\n",
        "\n",
        "            assert len(references) == len(hypotheses)\n",
        "\n",
        "        # Calculate BLEU-4 scores\n",
        "        bleu4 = corpus_bleu(references, hypotheses)\n",
        "\n",
        "        print(\n",
        "            '\\n * LOSS - {loss.avg:.3f}, TOP-5 ACCURACY - {top5.avg:.3f}, BLEU-4 - {bleu}\\n'.format(\n",
        "                loss=losses,\n",
        "                top5=top5accs,\n",
        "                bleu=bleu4))\n",
        "\n",
        "    return bleu4\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMz3PcBUIGQ6"
      },
      "source": [
        "## Start training *"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehs0beBuq_mM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "aa88c43e5d904fca8c0f2f3e28eab288",
            "28968311a417462b8979f8c8ea457928",
            "be6b2b50180b422ab541eadf3ad3d79b",
            "61a539cd9616496aa694801bd910d21f",
            "53bfe85c64fb4f79ae5f024f7898d2bf",
            "31120c22786e4d1b82d414c75c1ac290",
            "22c5a0bd033041fb99ff967353d3bfcf",
            "6bcc4137e51f47549d0a371bd0ec9ee5"
          ]
        },
        "outputId": "2e8a350b-0c49-4304-ea6d-57963492dc80"
      },
      "source": [
        "# Read word map\n",
        "word_map_file = os.path.join('word_map.json')\n",
        "with open(word_map_file, 'r') as j:\n",
        "    word_map = json.load(j)\n",
        "\n",
        "# Initialize encoder, and we don't train it\n",
        "encoder = Encoder()\n",
        "encoder_optimizer = None\n",
        "\n",
        "# Initialize decoder, and adam optimizer\n",
        "decoder = DecoderWithAttention(attention_dim=attention_dim,\n",
        "                                embed_dim=emb_dim,\n",
        "                                decoder_dim=decoder_dim,\n",
        "                                vocab_size=len(word_map),\n",
        "                                dropout=dropout)\n",
        "decoder_optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, decoder.parameters()),\n",
        "                                      lr=decoder_lr)\n",
        "\n",
        "\n",
        "# Move to GPU, if available\n",
        "decoder = decoder.to(device)\n",
        "encoder = encoder.to(device)\n",
        "\n",
        "# Loss function\n",
        "loss_function = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "# Custom dataloaders\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    CaptionDataset('train'),\n",
        "    batch_size=batch_size, \n",
        "    shuffle=True, \n",
        "    num_workers=1, \n",
        "    pin_memory=True\n",
        ")\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    CaptionDataset('val'),\n",
        "    batch_size=batch_size, \n",
        "    shuffle=True, \n",
        "    num_workers=1, \n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "# Epochs\n",
        "for epoch in range(epochs):\n",
        "    # One epoch's training\n",
        "    train(train_loader=train_loader,\n",
        "          encoder=encoder,\n",
        "          decoder=decoder,\n",
        "          loss_function=loss_function,\n",
        "          decoder_optimizer=decoder_optimizer,\n",
        "          epoch=epoch)\n",
        "\n",
        "    # One epoch's validation\n",
        "    recent_bleu4 = validate(val_loader=val_loader,\n",
        "                            encoder=encoder,\n",
        "                            decoder=decoder,\n",
        "                            loss_function=loss_function)\n",
        "\n",
        "    # Save checkpoint\n",
        "    is_best = recent_bleu4 > best_bleu4\n",
        "    best_bleu4 = max(recent_bleu4, best_bleu4)\n",
        "\n",
        "    save_checkpoint('coco', epoch, 0, encoder, decoder, encoder_optimizer,\n",
        "                    decoder_optimizer, recent_bleu4, is_best)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\" to /root/.cache/torch/hub/checkpoints/resnet101-5d3b4d8f.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "aa88c43e5d904fca8c0f2f3e28eab288",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=178728960.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: [0][0/126]\tLoss 11.2181 (11.2181)\tTop-5 Accuracy 0.000 (0.000)\n",
            "Epoch: [0][100/126]\tLoss 5.9757 (7.0003)\tTop-5 Accuracy 37.883 (33.418)\n",
            "Validation: [0/126]\tLoss 5.5259 (5.5259)\tTop-5 Accuracy 42.234 (42.234)\t\n",
            "Validation: [100/126]\tLoss 5.6910 (5.6864)\tTop-5 Accuracy 43.646 (41.719)\t\n",
            "\n",
            " * LOSS - 5.696, TOP-5 ACCURACY - 41.665, BLEU-4 - 0.019798742846755937\n",
            "\n",
            "Epoch: [1][0/126]\tLoss 5.9955 (5.9955)\tTop-5 Accuracy 39.437 (39.437)\n",
            "Epoch: [1][100/126]\tLoss 5.1200 (5.6183)\tTop-5 Accuracy 50.125 (44.326)\n",
            "Validation: [0/126]\tLoss 4.6494 (4.6494)\tTop-5 Accuracy 57.265 (57.265)\t\n",
            "Validation: [100/126]\tLoss 4.9912 (5.0468)\tTop-5 Accuracy 50.000 (51.468)\t\n",
            "\n",
            " * LOSS - 5.051, TOP-5 ACCURACY - 51.452, BLEU-4 - 0.0685348286502511\n",
            "\n",
            "Epoch: [2][0/126]\tLoss 5.1345 (5.1345)\tTop-5 Accuracy 50.000 (50.000)\n",
            "Epoch: [2][100/126]\tLoss 4.7788 (5.1082)\tTop-5 Accuracy 56.647 (50.961)\n",
            "Validation: [0/126]\tLoss 4.7505 (4.7505)\tTop-5 Accuracy 57.880 (57.880)\t\n",
            "Validation: [100/126]\tLoss 4.7280 (4.6619)\tTop-5 Accuracy 55.946 (57.067)\t\n",
            "\n",
            " * LOSS - 4.663, TOP-5 ACCURACY - 57.036, BLEU-4 - 0.11169926236862501\n",
            "\n",
            "Epoch: [3][0/126]\tLoss 4.8627 (4.8627)\tTop-5 Accuracy 57.542 (57.542)\n",
            "Epoch: [3][100/126]\tLoss 4.8166 (4.7720)\tTop-5 Accuracy 56.011 (55.056)\n",
            "Validation: [0/126]\tLoss 4.2195 (4.2195)\tTop-5 Accuracy 60.055 (60.055)\t\n",
            "Validation: [100/126]\tLoss 4.2359 (4.3658)\tTop-5 Accuracy 60.490 (60.392)\t\n",
            "\n",
            " * LOSS - 4.369, TOP-5 ACCURACY - 60.306, BLEU-4 - 0.13296753343673653\n",
            "\n",
            "Epoch: [4][0/126]\tLoss 4.5109 (4.5109)\tTop-5 Accuracy 56.868 (56.868)\n",
            "Epoch: [4][100/126]\tLoss 4.5011 (4.5268)\tTop-5 Accuracy 57.880 (58.135)\n",
            "Validation: [0/126]\tLoss 4.2728 (4.2728)\tTop-5 Accuracy 62.465 (62.465)\t\n",
            "Validation: [100/126]\tLoss 4.2767 (4.1327)\tTop-5 Accuracy 61.580 (63.626)\t\n",
            "\n",
            " * LOSS - 4.134, TOP-5 ACCURACY - 63.624, BLEU-4 - 0.1533416258653169\n",
            "\n",
            "Epoch: [5][0/126]\tLoss 4.4323 (4.4323)\tTop-5 Accuracy 59.884 (59.884)\n",
            "Epoch: [5][100/126]\tLoss 4.3541 (4.3161)\tTop-5 Accuracy 58.082 (60.863)\n",
            "Validation: [0/126]\tLoss 4.0876 (4.0876)\tTop-5 Accuracy 62.018 (62.018)\t\n",
            "Validation: [100/126]\tLoss 4.2487 (3.9207)\tTop-5 Accuracy 62.602 (65.873)\t\n",
            "\n",
            " * LOSS - 3.922, TOP-5 ACCURACY - 65.873, BLEU-4 - 0.1738880875925522\n",
            "\n",
            "Epoch: [6][0/126]\tLoss 4.0372 (4.0372)\tTop-5 Accuracy 63.408 (63.408)\n",
            "Epoch: [6][100/126]\tLoss 4.0633 (4.1243)\tTop-5 Accuracy 61.838 (63.131)\n",
            "Validation: [0/126]\tLoss 3.7464 (3.7464)\tTop-5 Accuracy 67.033 (67.033)\t\n",
            "Validation: [100/126]\tLoss 3.8531 (3.7438)\tTop-5 Accuracy 65.230 (68.239)\t\n",
            "\n",
            " * LOSS - 3.747, TOP-5 ACCURACY - 68.097, BLEU-4 - 0.19189494546216795\n",
            "\n",
            "Epoch: [7][0/126]\tLoss 4.1026 (4.1026)\tTop-5 Accuracy 62.169 (62.169)\n",
            "Epoch: [7][100/126]\tLoss 3.8845 (3.9574)\tTop-5 Accuracy 68.232 (65.212)\n",
            "Validation: [0/126]\tLoss 3.5166 (3.5166)\tTop-5 Accuracy 72.093 (72.093)\t\n",
            "Validation: [100/126]\tLoss 3.5582 (3.5824)\tTop-5 Accuracy 71.466 (70.469)\t\n",
            "\n",
            " * LOSS - 3.577, TOP-5 ACCURACY - 70.459, BLEU-4 - 0.21601866804584077\n",
            "\n",
            "Epoch: [8][0/126]\tLoss 3.8575 (3.8575)\tTop-5 Accuracy 66.839 (66.839)\n",
            "Epoch: [8][100/126]\tLoss 3.9505 (3.8138)\tTop-5 Accuracy 65.000 (66.980)\n",
            "Validation: [0/126]\tLoss 3.3387 (3.3387)\tTop-5 Accuracy 75.852 (75.852)\t\n",
            "Validation: [100/126]\tLoss 3.2209 (3.4258)\tTop-5 Accuracy 75.926 (72.251)\t\n",
            "\n",
            " * LOSS - 3.425, TOP-5 ACCURACY - 72.230, BLEU-4 - 0.23035052919142276\n",
            "\n",
            "Epoch: [9][0/126]\tLoss 3.6341 (3.6341)\tTop-5 Accuracy 68.912 (68.912)\n",
            "Epoch: [9][100/126]\tLoss 3.6110 (3.6708)\tTop-5 Accuracy 67.049 (68.557)\n",
            "Validation: [0/126]\tLoss 3.2205 (3.2205)\tTop-5 Accuracy 74.000 (74.000)\t\n",
            "Validation: [100/126]\tLoss 3.1743 (3.2671)\tTop-5 Accuracy 77.119 (74.560)\t\n",
            "\n",
            " * LOSS - 3.272, TOP-5 ACCURACY - 74.435, BLEU-4 - 0.2507551060561255\n",
            "\n",
            "Epoch: [10][0/126]\tLoss 3.4875 (3.4875)\tTop-5 Accuracy 71.703 (71.703)\n",
            "Epoch: [10][100/126]\tLoss 3.3542 (3.5454)\tTop-5 Accuracy 76.204 (70.340)\n",
            "Validation: [0/126]\tLoss 3.0002 (3.0002)\tTop-5 Accuracy 77.778 (77.778)\t\n",
            "Validation: [100/126]\tLoss 3.2466 (3.1443)\tTop-5 Accuracy 72.800 (75.760)\t\n",
            "\n",
            " * LOSS - 3.149, TOP-5 ACCURACY - 75.755, BLEU-4 - 0.2699980570679929\n",
            "\n",
            "Epoch: [11][0/126]\tLoss 3.6719 (3.6719)\tTop-5 Accuracy 67.131 (67.131)\n",
            "Epoch: [11][100/126]\tLoss 3.4783 (3.4286)\tTop-5 Accuracy 72.022 (71.995)\n",
            "Validation: [0/126]\tLoss 3.0157 (3.0157)\tTop-5 Accuracy 78.693 (78.693)\t\n",
            "Validation: [100/126]\tLoss 2.8683 (3.0208)\tTop-5 Accuracy 80.697 (77.935)\t\n",
            "\n",
            " * LOSS - 3.018, TOP-5 ACCURACY - 77.969, BLEU-4 - 0.2775900535673896\n",
            "\n",
            "Epoch: [12][0/126]\tLoss 3.4330 (3.4330)\tTop-5 Accuracy 72.846 (72.846)\n",
            "Epoch: [12][100/126]\tLoss 3.3510 (3.3101)\tTop-5 Accuracy 74.157 (73.676)\n",
            "Validation: [0/126]\tLoss 2.8253 (2.8253)\tTop-5 Accuracy 79.437 (79.437)\t\n",
            "Validation: [100/126]\tLoss 2.8751 (2.9143)\tTop-5 Accuracy 80.423 (79.715)\t\n",
            "\n",
            " * LOSS - 2.909, TOP-5 ACCURACY - 79.835, BLEU-4 - 0.30565701608093165\n",
            "\n",
            "Epoch: [13][0/126]\tLoss 3.1295 (3.1295)\tTop-5 Accuracy 76.471 (76.471)\n",
            "Epoch: [13][100/126]\tLoss 3.2515 (3.2091)\tTop-5 Accuracy 75.603 (75.059)\n",
            "Validation: [0/126]\tLoss 2.8590 (2.8590)\tTop-5 Accuracy 79.718 (79.718)\t\n",
            "Validation: [100/126]\tLoss 2.8690 (2.7835)\tTop-5 Accuracy 80.112 (81.751)\t\n",
            "\n",
            " * LOSS - 2.792, TOP-5 ACCURACY - 81.689, BLEU-4 - 0.3156898082691526\n",
            "\n",
            "Epoch: [14][0/126]\tLoss 3.1453 (3.1453)\tTop-5 Accuracy 76.694 (76.694)\n",
            "Epoch: [14][100/126]\tLoss 3.0524 (3.0938)\tTop-5 Accuracy 77.686 (76.991)\n",
            "Validation: [0/126]\tLoss 2.3818 (2.3818)\tTop-5 Accuracy 88.547 (88.547)\t\n",
            "Validation: [100/126]\tLoss 2.5935 (2.6886)\tTop-5 Accuracy 82.817 (83.711)\t\n",
            "\n",
            " * LOSS - 2.693, TOP-5 ACCURACY - 83.656, BLEU-4 - 0.3340178108618734\n",
            "\n",
            "Epoch: [15][0/126]\tLoss 3.0855 (3.0855)\tTop-5 Accuracy 78.630 (78.630)\n",
            "Epoch: [15][100/126]\tLoss 3.1395 (2.9973)\tTop-5 Accuracy 74.933 (78.410)\n",
            "Validation: [0/126]\tLoss 2.4663 (2.4663)\tTop-5 Accuracy 88.283 (88.283)\t\n",
            "Validation: [100/126]\tLoss 2.6502 (2.5805)\tTop-5 Accuracy 84.384 (85.605)\t\n",
            "\n",
            " * LOSS - 2.585, TOP-5 ACCURACY - 85.531, BLEU-4 - 0.35696060114160316\n",
            "\n",
            "Epoch: [16][0/126]\tLoss 2.8782 (2.8782)\tTop-5 Accuracy 79.943 (79.943)\n",
            "Epoch: [16][100/126]\tLoss 3.1231 (2.9064)\tTop-5 Accuracy 77.628 (80.037)\n",
            "Validation: [0/126]\tLoss 2.3863 (2.3863)\tTop-5 Accuracy 87.853 (87.853)\t\n",
            "Validation: [100/126]\tLoss 2.3795 (2.4992)\tTop-5 Accuracy 87.675 (86.816)\t\n",
            "\n",
            " * LOSS - 2.497, TOP-5 ACCURACY - 86.873, BLEU-4 - 0.3664137975782009\n",
            "\n",
            "Epoch: [17][0/126]\tLoss 2.9756 (2.9756)\tTop-5 Accuracy 78.747 (78.747)\n",
            "Epoch: [17][100/126]\tLoss 2.9049 (2.8351)\tTop-5 Accuracy 77.839 (81.179)\n",
            "Validation: [0/126]\tLoss 2.2226 (2.2226)\tTop-5 Accuracy 90.659 (90.659)\t\n",
            "Validation: [100/126]\tLoss 2.2875 (2.4306)\tTop-5 Accuracy 90.217 (88.519)\t\n",
            "\n",
            " * LOSS - 2.422, TOP-5 ACCURACY - 88.683, BLEU-4 - 0.382807353631336\n",
            "\n",
            "Epoch: [18][0/126]\tLoss 2.7379 (2.7379)\tTop-5 Accuracy 83.029 (83.029)\n",
            "Epoch: [18][100/126]\tLoss 2.8227 (2.7364)\tTop-5 Accuracy 81.096 (83.129)\n",
            "Validation: [0/126]\tLoss 2.3928 (2.3928)\tTop-5 Accuracy 88.202 (88.202)\t\n",
            "Validation: [100/126]\tLoss 2.3486 (2.3317)\tTop-5 Accuracy 88.728 (89.991)\t\n",
            "\n",
            " * LOSS - 2.329, TOP-5 ACCURACY - 90.019, BLEU-4 - 0.3890259531356273\n",
            "\n",
            "Epoch: [19][0/126]\tLoss 2.6279 (2.6279)\tTop-5 Accuracy 86.016 (86.016)\n",
            "Epoch: [19][100/126]\tLoss 2.7753 (2.6653)\tTop-5 Accuracy 83.611 (84.340)\n",
            "Validation: [0/126]\tLoss 2.3740 (2.3740)\tTop-5 Accuracy 89.390 (89.390)\t\n",
            "Validation: [100/126]\tLoss 2.3167 (2.2484)\tTop-5 Accuracy 90.761 (91.692)\t\n",
            "\n",
            " * LOSS - 2.252, TOP-5 ACCURACY - 91.602, BLEU-4 - 0.4251561912876993\n",
            "\n",
            "Epoch: [20][0/126]\tLoss 2.7587 (2.7587)\tTop-5 Accuracy 82.725 (82.725)\n",
            "Epoch: [20][100/126]\tLoss 2.5995 (2.5875)\tTop-5 Accuracy 86.027 (85.506)\n",
            "Validation: [0/126]\tLoss 2.1340 (2.1340)\tTop-5 Accuracy 93.352 (93.352)\t\n",
            "Validation: [100/126]\tLoss 2.3521 (2.1787)\tTop-5 Accuracy 89.600 (92.873)\t\n",
            "\n",
            " * LOSS - 2.179, TOP-5 ACCURACY - 92.868, BLEU-4 - 0.4384772591559174\n",
            "\n",
            "Epoch: [21][0/126]\tLoss 2.5028 (2.5028)\tTop-5 Accuracy 86.436 (86.436)\n",
            "Epoch: [21][100/126]\tLoss 2.6350 (2.5094)\tTop-5 Accuracy 86.040 (87.132)\n",
            "Validation: [0/126]\tLoss 1.9602 (1.9602)\tTop-5 Accuracy 95.954 (95.954)\t\n",
            "Validation: [100/126]\tLoss 2.0693 (2.1098)\tTop-5 Accuracy 93.915 (93.766)\t\n",
            "\n",
            " * LOSS - 2.112, TOP-5 ACCURACY - 93.752, BLEU-4 - 0.4592835114597112\n",
            "\n",
            "Epoch: [22][0/126]\tLoss 2.4486 (2.4486)\tTop-5 Accuracy 86.908 (86.908)\n",
            "Epoch: [22][100/126]\tLoss 2.3643 (2.4540)\tTop-5 Accuracy 88.594 (88.127)\n",
            "Validation: [0/126]\tLoss 2.0471 (2.0471)\tTop-5 Accuracy 95.116 (95.116)\t\n",
            "Validation: [100/126]\tLoss 2.1512 (2.0544)\tTop-5 Accuracy 91.892 (94.589)\t\n",
            "Epoch: [23][0/126]\tLoss 2.4539 (2.4539)\tTop-5 Accuracy 88.657 (88.657)\n",
            "Epoch: [23][100/126]\tLoss 2.3173 (2.3903)\tTop-5 Accuracy 89.665 (89.119)\n",
            "Validation: [0/126]\tLoss 2.0117 (2.0117)\tTop-5 Accuracy 94.413 (94.413)\t\n",
            "Validation: [100/126]\tLoss 1.9571 (1.9875)\tTop-5 Accuracy 95.368 (95.284)\t\n",
            "\n",
            " * LOSS - 1.984, TOP-5 ACCURACY - 95.322, BLEU-4 - 0.5087501557342029\n",
            "\n",
            "Epoch: [24][0/126]\tLoss 2.1492 (2.1492)\tTop-5 Accuracy 91.247 (91.247)\n",
            "Epoch: [24][100/126]\tLoss 2.4670 (2.3325)\tTop-5 Accuracy 86.207 (89.946)\n",
            "Validation: [0/126]\tLoss 1.8961 (1.8961)\tTop-5 Accuracy 96.409 (96.409)\t\n",
            "Validation: [100/126]\tLoss 1.9912 (1.9256)\tTop-5 Accuracy 93.085 (95.904)\t\n",
            "\n",
            " * LOSS - 1.922, TOP-5 ACCURACY - 95.942, BLEU-4 - 0.5288159641992745\n",
            "\n",
            "Epoch: [25][0/126]\tLoss 2.3215 (2.3215)\tTop-5 Accuracy 88.859 (88.859)\n",
            "Epoch: [25][100/126]\tLoss 2.2748 (2.2761)\tTop-5 Accuracy 91.169 (90.663)\n",
            "Validation: [0/126]\tLoss 1.9468 (1.9468)\tTop-5 Accuracy 95.101 (95.101)\t\n",
            "Validation: [100/126]\tLoss 1.9349 (1.8806)\tTop-5 Accuracy 97.820 (96.451)\t\n",
            "\n",
            " * LOSS - 1.881, TOP-5 ACCURACY - 96.385, BLEU-4 - 0.5420111040317177\n",
            "\n",
            "Epoch: [26][0/126]\tLoss 2.2055 (2.2055)\tTop-5 Accuracy 91.200 (91.200)\n",
            "Epoch: [26][100/126]\tLoss 2.3052 (2.2242)\tTop-5 Accuracy 89.835 (91.668)\n",
            "Validation: [0/126]\tLoss 1.7620 (1.7620)\tTop-5 Accuracy 96.758 (96.758)\t\n",
            "Validation: [100/126]\tLoss 1.8033 (1.8203)\tTop-5 Accuracy 97.436 (96.769)\t\n",
            "\n",
            " * LOSS - 1.821, TOP-5 ACCURACY - 96.784, BLEU-4 - 0.5669470246738662\n",
            "\n",
            "Epoch: [27][0/126]\tLoss 2.1889 (2.1889)\tTop-5 Accuracy 92.818 (92.818)\n",
            "Epoch: [27][100/126]\tLoss 2.1851 (2.1823)\tTop-5 Accuracy 91.892 (92.270)\n",
            "Validation: [0/126]\tLoss 1.7437 (1.7437)\tTop-5 Accuracy 96.429 (96.429)\t\n",
            "Validation: [100/126]\tLoss 1.7935 (1.7809)\tTop-5 Accuracy 96.667 (97.171)\t\n",
            "\n",
            " * LOSS - 1.778, TOP-5 ACCURACY - 97.221, BLEU-4 - 0.588457005671952\n",
            "\n",
            "Epoch: [28][0/126]\tLoss 2.0493 (2.0493)\tTop-5 Accuracy 94.477 (94.477)\n",
            "Epoch: [28][100/126]\tLoss 2.0783 (2.1220)\tTop-5 Accuracy 93.207 (93.081)\n",
            "Validation: [0/126]\tLoss 1.7963 (1.7963)\tTop-5 Accuracy 97.442 (97.442)\t\n",
            "Validation: [100/126]\tLoss 1.6860 (1.7227)\tTop-5 Accuracy 97.989 (97.772)\t\n",
            "\n",
            " * LOSS - 1.724, TOP-5 ACCURACY - 97.749, BLEU-4 - 0.6087232523301209\n",
            "\n",
            "Epoch: [29][0/126]\tLoss 1.9927 (1.9927)\tTop-5 Accuracy 96.629 (96.629)\n",
            "Epoch: [29][100/126]\tLoss 2.3384 (2.0814)\tTop-5 Accuracy 88.978 (93.468)\n",
            "Validation: [0/126]\tLoss 1.6729 (1.6729)\tTop-5 Accuracy 98.193 (98.193)\t\n",
            "Validation: [100/126]\tLoss 1.6388 (1.6796)\tTop-5 Accuracy 98.087 (98.037)\t\n",
            "\n",
            " * LOSS - 1.681, TOP-5 ACCURACY - 98.020, BLEU-4 - 0.6304448457157986\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jG4Ydk3B9oL"
      },
      "source": [
        "!cp BEST_checkpoint_coco.pth.tar /tmp/BEST_checkpoint_coco.pth.tar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhcZ5iHVAB6X",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "3364bf65-5258-4293-8824-cb8be9bda3d6"
      },
      "source": [
        "!curl https://bashupload.com/BEST_checkpoint_coco.pth.tar --data-binary @/tmp/BEST_checkpoint_coco.pth.tar"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tcmalloc: large alloc 1073750016 bytes == 0x561298612000 @  0x7f94363362a4 0x561255d08c81 0x561255d017bc 0x561255d03098 0x561255d083d8 0x561255cfb5d0 0x7f9435877b97 0x561255cfb71a\n",
            "\n",
            "Uploaded 1 file, 631369728 bytes\n",
            "\n",
            "wget https://bashupload.com/gZLIi/IG9DU.tar\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i06Y3lfF0eB0"
      },
      "source": [
        "# Inference\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPXPswX3oAl5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MdKXePb5oB0f"
      },
      "source": [
        "!cp /content/drive/My\\ Drive/bootcamp/ML\\ fundamentals/image\\ captioning/BEST_checkpoint_coco.pth.tar ./\n",
        "!cp /content/drive/My\\ Drive/bootcamp/ML\\ fundamentals/image\\ captioning/word_map.json ./"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_TAI_mMJoLr8"
      },
      "source": [
        "# Read word map\n",
        "word_map_file = os.path.join('word_map.json')\n",
        "with open(word_map_file, 'r') as j:\n",
        "    word_map = json.load(j)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1cgA_Cz-NHH"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import h5py\n",
        "import json\n",
        "import torch\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "from random import seed, choice, sample\n",
        "import cv2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3C5wGoKK_k3"
      },
      "source": [
        "import torch.nn.functional as F\n",
        "vocab_size = len(word_map)\n",
        "def inference(img, allcaps):\n",
        "  references = list()\n",
        "  hypotheses = list()\n",
        "\n",
        "  checkpoint = torch.load('BEST_checkpoint_coco.pth.tar')\n",
        "  decoder = checkpoint['decoder']\n",
        "  decoder = decoder.to(device)\n",
        "  decoder.eval()\n",
        "  encoder = checkpoint['encoder']\n",
        "  encoder = encoder.to(device)\n",
        "  encoder.eval()\n",
        "  normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                    std=[0.229, 0.224, 0.225])\n",
        "\n",
        "  k = 3\n",
        "  encoder_out = encoder(img.reshape([1,3,256,256]).to(device))  # (1, enc_image_size, enc_image_size, encoder_dim)\n",
        "  enc_image_size = encoder_out.size(1)\n",
        "  encoder_dim = encoder_out.size(3)\n",
        "  encoder_out = encoder_out.view(1, -1, encoder_dim)  # (1, num_pixels, encoder_dim)\n",
        "  num_pixels = encoder_out.size(1) \n",
        "\n",
        "  # We'll treat the problem as having a batch size of k\n",
        "  encoder_out = encoder_out.expand(k, num_pixels, encoder_dim)  # (k, num_pixels, encoder_dim)\n",
        "\n",
        "  # Tensor to store top k previous words at each step; now they're just <start>\n",
        "  k_prev_words = torch.LongTensor([[word_map['<start>']]] * k).to(device)  # (k, 1)\n",
        "\n",
        "  # Tensor to store top k sequences; now they're just <start>\n",
        "  seqs = k_prev_words  # (k, 1)\n",
        "\n",
        "  # Tensor to store top k sequences' scores; now they're just 0\n",
        "  top_k_scores = torch.zeros(k, 1).to(device)  # (k, 1)\n",
        "  # Lists to store completed sequences and scores\n",
        "  complete_seqs = list()\n",
        "  complete_seqs_scores = list()\n",
        "\n",
        "  # Start decoding\n",
        "  step = 1\n",
        "  h, c = decoder.init_hidden_state(encoder_out)\n",
        "\n",
        "  # s is a number less than or equal to k, because sequences are removed from this process once they hit <end>\n",
        "  while True:\n",
        "\n",
        "      embeddings = decoder.embedding(k_prev_words).squeeze(1)  # (s, embed_dim)\n",
        "\n",
        "      awe, _ = decoder.attention(encoder_out, h)  # (s, encoder_dim), (s, num_pixels)\n",
        "\n",
        "      gate = decoder.sigmoid(decoder.f_beta(h))  # gating scalar, (s, encoder_dim)\n",
        "      awe = gate * awe\n",
        "\n",
        "      h, c = decoder.decode_step(torch.cat([embeddings, awe], dim=1), (h, c))  # (s, decoder_dim)\n",
        "\n",
        "      scores = decoder.fc(h)  # (s, vocab_size)\n",
        "      scores = F.log_softmax(scores, dim=1)\n",
        "\n",
        "      # Add\n",
        "      scores = top_k_scores.expand_as(scores) + scores  # (s, vocab_size)\n",
        "\n",
        "      # For the first step, all k points will have the same scores (since same k previous words, h, c)\n",
        "      if step == 1:\n",
        "          top_k_scores, top_k_words = scores[0].topk(k, 0, True, True)  # (s)\n",
        "      else:\n",
        "          # Unroll and find top scores, and their unrolled indices\n",
        "          top_k_scores, top_k_words = scores.view(-1).topk(k, 0, True, True)  # (s)\n",
        "\n",
        "      # Convert unrolled indices to actual indices of scores\n",
        "      prev_word_inds = top_k_words // vocab_size  # (s)\n",
        "      next_word_inds = top_k_words % vocab_size  # (s)\n",
        "\n",
        "      # Add new words to sequences\n",
        "      seqs = torch.cat([seqs[prev_word_inds], next_word_inds.unsqueeze(1)], dim=1)  # (s, step+1)\n",
        "\n",
        "      # Which sequences are incomplete (didn't reach <end>)?\n",
        "      incomplete_inds = [ind for ind, next_word in enumerate(next_word_inds) if\n",
        "                          next_word != word_map['<end>']]\n",
        "      complete_inds = list(set(range(len(next_word_inds))) - set(incomplete_inds))\n",
        "\n",
        "      # Set aside complete sequences\n",
        "      if len(complete_inds) > 0:\n",
        "          complete_seqs.extend(seqs[complete_inds].tolist())\n",
        "          complete_seqs_scores.extend(top_k_scores[complete_inds])\n",
        "      k -= len(complete_inds)  # reduce beam length accordingly\n",
        "\n",
        "      # Proceed with incomplete sequences\n",
        "      if k == 0:\n",
        "          break\n",
        "      seqs = seqs[incomplete_inds]\n",
        "      h = h[prev_word_inds[incomplete_inds]]\n",
        "      c = c[prev_word_inds[incomplete_inds]]\n",
        "      encoder_out = encoder_out[prev_word_inds[incomplete_inds]]\n",
        "      top_k_scores = top_k_scores[incomplete_inds].unsqueeze(1)\n",
        "      k_prev_words = next_word_inds[incomplete_inds].unsqueeze(1)\n",
        "\n",
        "      # Break if things have been going on too long\n",
        "      if step > 50:\n",
        "          break\n",
        "      step += 1\n",
        "\n",
        "  i = complete_seqs_scores.index(max(complete_seqs_scores))\n",
        "  seq = complete_seqs[i]\n",
        "\n",
        "  # References\n",
        "  img_caps = allcaps[0].tolist()\n",
        "  img_captions = list(\n",
        "      map(lambda c: [w for w in c if w not in {word_map['<start>'], word_map['<end>'], word_map['<pad>']}],\n",
        "          img_caps))  # remove <start> and pads\n",
        "  references.append(img_captions)\n",
        "\n",
        "  # Hypotheses\n",
        "  hypotheses.append([w for w in seq if w not in {word_map['<start>'], word_map['<end>'], word_map['<pad>']}])\n",
        "  print('Model prediction:')\n",
        "  for ref in references[0]:\n",
        "    print(' '.join([rev_word_map[num] for num in ref]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpey8cUNGGDf"
      },
      "source": [
        "import matplotlib. pyplot as plt \n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    CaptionDataset('val'),\n",
        "    batch_size=batch_size, \n",
        "    shuffle=True, \n",
        "    num_workers=1, \n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "rev_word_map = {v: k for k, v in word_map.items()}\n",
        "\n",
        "for i, (imgs, caps, caplens, allcaps) in enumerate(val_loader):\n",
        "  img = imgs[0].detach().numpy().transpose([1,2,0])\n",
        "  normalized_img = (img-np.min(img))/(np.max(img)-np.min(img))\n",
        "  plt.imshow(cv2.cvtColor(normalized_img, cv2.COLOR_BGR2RGB))\n",
        "  plt.show()\n",
        "  print('Label:')\n",
        "  sentence = []\n",
        "  for num in caps[0]:\n",
        "    word = rev_word_map[int(num.numpy())]\n",
        "    sentence.append(word)\n",
        "    if word == '<end>': break\n",
        "  print(' '.join(sentence))\n",
        "  inference(imgs[0], allcaps)\n",
        "  \n",
        "  break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9smGtXpLPNTT"
      },
      "source": [
        "!pip install ffmpeg-python\n",
        "import ffmpeg"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}