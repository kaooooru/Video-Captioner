{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Copy of Model-training-medium",
      "provenance": [],
      "collapsed_sections": [
        "9Va2JtIOomk0",
        "NPu-GjhgqnkY",
        "LLwXhQwCHIb3",
        "loGag7jyHM-1",
        "mjef7jONHOX6",
        "Gddi9-OWLkqT",
        "SCdA_a6PJBE2",
        "YcsBgmB6H6C6",
        "eWgvfAFLH_ov",
        "sMz3PcBUIGQ6"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MK5ZR5XE5DhL"
      },
      "source": [
        "# Download dataset *"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KPW_aMg6eXa"
      },
      "source": [
        "!wget https://bashupload.com/-nc5I/train2014.zip\n",
        "!unzip train2014.zip\n",
        "!rm train2014.zip\n",
        "!wget https://bashupload.com/baIo3/val2014.zip\n",
        "!unzip val2014.zip\n",
        "!rm val2014.zip\n",
        "!wget http://cs.stanford.edu/people/karpathy/deepimagesent/caption_datasets.zip\n",
        "!unzip caption_datasets.zip\n",
        "!rm caption_datasets.zip\n",
        "!rm dataset_flickr30k.json\n",
        "!rm dataset_flickr8k.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPxir775dbR2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c0b22857-c027-4525-a7c2-f7665339509d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Va2JtIOomk0"
      },
      "source": [
        "#Data preprocessing *"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92LVl2M6q-NH"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import h5py\n",
        "import json\n",
        "import torch\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "from random import seed, choice, sample\n",
        "import cv2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5Z5vtSSnnLN"
      },
      "source": [
        "# Datasets:\n",
        "#   train2014 is a folder of image files for training\n",
        "#   val2014 is a folder of image files for validation\n",
        "#   dataset_coco.json is a JSON file that tells you {image -> captions}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78RzSgTmo1eL"
      },
      "source": [
        "## Data loading *\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnHihsol6lno"
      },
      "source": [
        "# Load JSON file into dict\n",
        "json_path = 'dataset_coco.json'\n",
        "with open(json_path, 'r') as j:\n",
        "    data = json.load(j)\n",
        "print(data['images'][0])\n",
        "\n",
        "# Understand how each image is captioned\n",
        "# 'filename' is the image name\n",
        "# 'filepath' is the folder name\n",
        "# 'imgid' is the id of the image\n",
        "# 'sentences' is a list of the human captioning\n",
        "# 'tokens' is a list of words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9lZ0WbUO6pj2"
      },
      "source": [
        "data[\"images\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKGEnilkqXPf"
      },
      "source": [
        "# Each image may have multiple captions\n",
        "# to reduce the bias we are introducing, \n",
        "# let's use the same number of captions per image\n",
        "captions_per_image=5\n",
        "\n",
        "# Maximum number of words in a sentence\n",
        "# If the sentence has more than max_len words, we skip it\n",
        "# If the sentence has less than max_len words, we pad it with <pad>\n",
        "max_len=50\n",
        "\n",
        "# From json object to a list of (image_path, captions) pairs \n",
        "# note: captions should be a list of word lists\n",
        "train_img_cap_pairs = []\n",
        "val_img_cap_pairs = []\n",
        "# test_img_cap_pairs = []\n",
        "\n",
        "# It contains all distinct words\n",
        "word_set = set()\n",
        "\n",
        "for img_obj in data['images']:\n",
        "    captions = []\n",
        "    for caption in img_obj['sentences']:\n",
        "        word_set.update(caption['tokens'])\n",
        "        if len(caption['tokens']) <= max_len:\n",
        "            captions.append(caption['tokens'])\n",
        "\n",
        "    # If captions is empty, what should we do here?\n",
        "    if len(captions) == 0:\n",
        "        continue\n",
        "\n",
        "    img_path = os.path.join(img_obj['filepath'], img_obj['filename'])\n",
        "\n",
        "    # What if this image cannot be found?\n",
        "    if not os.path.exists(img_path): continue\n",
        "\n",
        "    # Append the pair to the list\n",
        "    if img_obj['split'] == 'train':\n",
        "      train_img_cap_pairs.append([img_path, captions])\n",
        "    elif img_obj['split'] == 'val':\n",
        "      val_img_cap_pairs.append([img_path, captions])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qay9wsqmvYFq"
      },
      "source": [
        "## Data tranformation *"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpnovM8wvYfY"
      },
      "source": [
        "# HDF5: HDF5 is a unique technology suite that makes possible the management\n",
        "# of extremely large and complex data collections.\n",
        "\n",
        "# 1. We will create 2 hdf5 files: \n",
        "#      train_images.hdf5, val_images.hdf5\n",
        "# 2. We will create 5 json files: \n",
        "#      word_map.json -- contains a (word -> number) hash object\n",
        "#      train_captions.json -- contains a list of encoded training captions\n",
        "#      val_captions.json -- contains a list of encoded validation captions\n",
        "#      train_caption_length.json -- contains a list of training caption lengths\n",
        "#      val_caption_length.json -- contains a list of validation caption lengths"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lU4bEZ-Ww8Bu"
      },
      "source": [
        "# Word Encoding\n",
        "# word_map: word    -> number (starting from 1)\n",
        "#           <pad>   -> 0\n",
        "#           <start> -> the second highest number\n",
        "#           <end>   -> the highest number\n",
        "word_map = {k: idx + 1 for idx, k in enumerate(word_set)}\n",
        "word_map['<start>'] = len(word_map) + 1\n",
        "word_map['<end>'] = len(word_map) + 1\n",
        "word_map['<pad>'] = 0\n",
        "\n",
        "# Save word map to a JSON\n",
        "with open(os.path.join('word_map.json'), 'w') as j:\n",
        "  json.dump(word_map, j)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k8RI7wgs6u4s"
      },
      "source": [
        "for img_cap_pairs, split in [[train_img_cap_pairs,'train'], [val_img_cap_pairs, 'val']]:\n",
        "    # Save encoded captions and their lengths to JSON files\n",
        "    h5py_path = os.path.join(split + '_images.hdf5')\n",
        "    \n",
        "    # remove it if the path exists\n",
        "    if os.path.exists(h5py_path): os.remove(h5py_path)\n",
        "\n",
        "    with h5py.File(h5py_path, 'a') as h:\n",
        "        # Make a note of the number of captions we are sampling per image\n",
        "        h.attrs['captions_per_image'] = captions_per_image\n",
        "\n",
        "        # Create dataset inside HDF5 file to store images\n",
        "        # we do channel first for the image\n",
        "        images = h.create_dataset('images', (len(img_cap_pairs), 3, 256, 256), dtype='uint8')\n",
        "\n",
        "        enc_captions = []\n",
        "        caplens = []\n",
        "        for index, img_cap_pair in enumerate(img_cap_pairs):\n",
        "            img_path, captions = img_cap_pair\n",
        "\n",
        "            if len(captions) < captions_per_image:\n",
        "                # add some captions by randomly sampling from captions\n",
        "                captions = captions + [choice(captions) for _ in range(captions_per_image - len(captions))]\n",
        "            else:\n",
        "                # randomly sample k from captions\n",
        "                captions = sample(captions, captions_per_image)\n",
        "\n",
        "            # Sanity check\n",
        "            assert len(captions) == captions_per_image\n",
        "\n",
        "            # Read image and transform it into (3, 256, 256)\n",
        "            # Hint: use cv2, you will need to read, resize and transpose\n",
        "            img = cv2.imread(img_path)\n",
        "            img = cv2.resize(img, (256, 256))\n",
        "            img = img.transpose(2, 0, 1)\n",
        "\n",
        "            assert img.shape == (3, 256, 256)\n",
        "\n",
        "            # Save image to HDF5 file\n",
        "            images[index] = img\n",
        "            for idx, caption in enumerate(captions):\n",
        "                # Encode captions\n",
        "                #   a list of numbers\n",
        "                #   Format should be <start> word1 word2 ... wordN <end> <pad> <pad>...\n",
        "                #   The total length should be equal to max_len\n",
        "                enc_c = [word_map['<start>']] + [word_map[word] for word in caption] + \\\n",
        "                 [word_map['<end>']] + [word_map['<pad>']] * (max_len - len(caption))\n",
        "\n",
        "                enc_captions.append(enc_c)\n",
        "                caplens.append(len(caption) + 2)                \n",
        "\n",
        "    with open(os.path.join(split + '_captions.json'), 'w') as j:\n",
        "        json.dump(enc_captions, j)\n",
        "\n",
        "    with open(os.path.join(split + '_caption_length.json'), 'w') as j:\n",
        "        json.dump(caplens, j)\n",
        "\n",
        "# Sanity check\n",
        "print('caption length:', caplens[-1])\n",
        "print('caption:', caption)\n",
        "print('caption encoding:', enc_c)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPu-GjhgqnkY"
      },
      "source": [
        "#Helper functions *"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Br2WmVaQl8zd"
      },
      "source": [
        "def save_checkpoint(data_name, epoch, epochs_since_improvement, encoder, decoder, encoder_optimizer, decoder_optimizer,\n",
        "                    bleu4, is_best):\n",
        "    state = {'epoch': epoch,\n",
        "             'epochs_since_improvement': epochs_since_improvement,\n",
        "             'bleu-4': bleu4,\n",
        "             'encoder': encoder,\n",
        "             'decoder': decoder,\n",
        "             'encoder_optimizer': encoder_optimizer,\n",
        "             'decoder_optimizer': decoder_optimizer}\n",
        "    filename = 'checkpoint_' + data_name + '.pth.tar'\n",
        "    torch.save(state, filename)\n",
        "    # If this checkpoint is the best so far, store a copy so it doesn't get overwritten by a worse checkpoint\n",
        "    if is_best:\n",
        "        torch.save(state, 'BEST_' + filename)\n",
        "\n",
        "\n",
        "class AverageMeter(object):\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "def accuracy(scores, targets, k):\n",
        "    batch_size = targets.size(0)\n",
        "    _, ind = scores.topk(k, 1, True, True)\n",
        "    correct = ind.eq(targets.view(-1, 1).expand_as(ind))\n",
        "    correct_total = correct.view(-1).float().sum()  # 0D tensor\n",
        "    return correct_total.item() * (100.0 / batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fnKTTtm5Ru2"
      },
      "source": [
        "# Model Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLwXhQwCHIb3"
      },
      "source": [
        "## Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXd4HvZ0qmw2"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torchvision\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, encoded_image_size=14):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.enc_image_size = encoded_image_size\n",
        "\n",
        "        resnet = torchvision.models.resnet101(pretrained=True)   # pretrained ImageNet ResNet-101\n",
        "\n",
        "        # Remove linear and pool layers. Do you know why? (linear: we don't need to classify, we just want encoder pool: we want to change image size)\n",
        "        modules = nn.Sequential(*(list(resnet.children())[:-2]))\n",
        "        self.resnet = modules\n",
        "\n",
        "        # Resize image to fixed size to allow input images of variable size\n",
        "        self.adaptive_pool = nn.AdaptiveAvgPool2d(self.enc_image_size)\n",
        "\n",
        "        # We want to avoid training encoder\n",
        "        for param in resnet.parameters():\n",
        "          param.requires_grad = False\n",
        "\n",
        "    def forward(self, images):\n",
        "        # Use resnet, apply adaptive_pool\n",
        "        out = self.resnet(images)  # (batch_size, 2048, image_size/32, image_size/32)\n",
        "        out = self.adaptive_pool(out)  # (batch_size, 2048, encoded_image_size, encoded_image_size)\n",
        "        out = out.permute(0,2,3,1)  # (batch_size, encoded_image_size, encoded_image_size, 2048)\n",
        "        return out\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loGag7jyHM-1"
      },
      "source": [
        "##Attention *"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nd2Gpxu0HP2h"
      },
      "source": [
        "class Attention(nn.Module):\n",
        "    \"\"\"\n",
        "    Attention Network.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n",
        "        \"\"\"\n",
        "        :param encoder_dim: feature size of encoded images\n",
        "        :param decoder_dim: size of decoder's RNN\n",
        "        :param attention_dim: size of the attention network\n",
        "        \"\"\"\n",
        "        super(Attention, self).__init__()\n",
        "        self.encoder_att = nn.Linear(encoder_dim, attention_dim)  # linear layer to transform encoded image\n",
        "        self.decoder_att = nn.Linear(decoder_dim, attention_dim)  # linear layer to transform decoder's output\n",
        "        self.full_att = nn.Linear(attention_dim, 1)  # linear layer to calculate values to be softmax-ed\n",
        "        self.relu = nn.ReLU()\n",
        "        self.softmax = nn.Softmax(dim=1)  # softmax layer to calculate weights\n",
        "\n",
        "    def forward(self, encoder_out, decoder_hidden):\n",
        "        \"\"\"\n",
        "        Forward propagation.\n",
        "        :param encoder_out: encoded images, a tensor of dimension (batch_size, num_pixels, encoder_dim)\n",
        "        :param decoder_hidden: previous decoder output, a tensor of dimension (batch_size, decoder_dim)\n",
        "        :return: attention weighted encoding, weights\n",
        "        \"\"\"\n",
        "        att1 = self.encoder_att(encoder_out)  # (batch_size, num_pixels, attention_dim)\n",
        "        att2 = self.decoder_att(decoder_hidden)  # (batch_size, attention_dim)\n",
        "        att = self.full_att(self.relu(att1 + att2.unsqueeze(1))).squeeze(2)  # (batch_size, num_pixels)\n",
        "        alpha = self.softmax(att)  # (batch_size, num_pixels)\n",
        "        attention_weighted_encoding = (encoder_out * alpha.unsqueeze(2)).sum(dim=1)  # (batch_size, encoder_dim)\n",
        "\n",
        "        return attention_weighted_encoding, alpha\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjef7jONHOX6"
      },
      "source": [
        "##Decoder *"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_N6-RqyLHUvt"
      },
      "source": [
        "\n",
        "class DecoderWithAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Decoder.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, attention_dim, embed_dim, decoder_dim, vocab_size, encoder_dim=2048, dropout=0.5):\n",
        "        \"\"\"\n",
        "        :param attention_dim: size of attention network\n",
        "        :param embed_dim: embedding size\n",
        "        :param decoder_dim: size of decoder's RNN\n",
        "        :param vocab_size: size of vocabulary\n",
        "        :param encoder_dim: feature size of encoded images\n",
        "        :param dropout: dropout\n",
        "        \"\"\"\n",
        "        super(DecoderWithAttention, self).__init__()\n",
        "\n",
        "        self.encoder_dim = encoder_dim\n",
        "        self.attention_dim = attention_dim\n",
        "        self.embed_dim = embed_dim\n",
        "        self.decoder_dim = decoder_dim\n",
        "        self.vocab_size = vocab_size\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.attention = Attention(encoder_dim, decoder_dim, attention_dim)  # attention network\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)  # embedding layer\n",
        "        self.dropout = nn.Dropout(p=self.dropout)\n",
        "        self.decode_step = nn.LSTMCell(embed_dim + encoder_dim, decoder_dim, bias=True)  # decoding LSTMCell\n",
        "        self.init_h = nn.Linear(encoder_dim, decoder_dim)  # linear layer to find initial hidden state of LSTMCell\n",
        "        self.init_c = nn.Linear(encoder_dim, decoder_dim)  # linear layer to find initial cell state of LSTMCell\n",
        "        self.f_beta = nn.Linear(decoder_dim, encoder_dim)  # linear layer to create a sigmoid-activated gate\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.fc = nn.Linear(decoder_dim, vocab_size)  # linear layer to find scores over vocabulary\n",
        "        self.init_weights()  # initialize some layers with the uniform distribution\n",
        "\n",
        "    def init_weights(self):\n",
        "        \"\"\"\n",
        "        Initializes some parameters with values from the uniform distribution, for easier convergence.\n",
        "        \"\"\"\n",
        "        self.embedding.weight.data.uniform_(-0.1, 0.1)\n",
        "        self.fc.bias.data.fill_(0)\n",
        "        self.fc.weight.data.uniform_(-0.1, 0.1)\n",
        "\n",
        "    def init_hidden_state(self, encoder_out):\n",
        "        \"\"\"\n",
        "        Creates the initial hidden and cell states for the decoder's LSTM based on the encoded images.\n",
        "        :param encoder_out: encoded images, a tensor of dimension (batch_size, num_pixels, encoder_dim)\n",
        "        :return: hidden state, cell state\n",
        "        \"\"\"\n",
        "        mean_encoder_out = encoder_out.mean(dim=1)\n",
        "        h = self.init_h(mean_encoder_out)  # (batch_size, decoder_dim)\n",
        "        c = self.init_c(mean_encoder_out)\n",
        "        return h, c\n",
        "\n",
        "    def forward(self, encoder_out, encoded_captions, caption_lengths):\n",
        "        \"\"\"\n",
        "        Forward propagation.\n",
        "        :param encoder_out: encoded images, a tensor of dimension (batch_size, enc_image_size, enc_image_size, encoder_dim)\n",
        "        :param encoded_captions: encoded captions, a tensor of dimension (batch_size, max_caption_length)\n",
        "        :param caption_lengths: caption lengths, a tensor of dimension (batch_size, 1)\n",
        "        :return: scores for vocabulary, sorted encoded captions, decode lengths, weights, sort indices\n",
        "        \"\"\"\n",
        "\n",
        "        batch_size = encoder_out.size(0)\n",
        "        encoder_dim = encoder_out.size(-1)\n",
        "        vocab_size = self.vocab_size\n",
        "\n",
        "        # Flatten image\n",
        "        encoder_out = encoder_out.view(batch_size, -1, encoder_dim)  # (batch_size, num_pixels, encoder_dim)\n",
        "        num_pixels = encoder_out.size(1)\n",
        "\n",
        "        # Sort input data by decreasing lengths; why? apparent below\n",
        "        caption_lengths, sort_ind = caption_lengths.squeeze(1).sort(dim=0, descending=True)\n",
        "        encoder_out = encoder_out[sort_ind]\n",
        "        encoded_captions = encoded_captions[sort_ind]\n",
        "\n",
        "        # Embedding\n",
        "        embeddings = self.embedding(encoded_captions)  # (batch_size, max_caption_length, embed_dim)\n",
        "\n",
        "        # Initialize LSTM state\n",
        "        h, c = self.init_hidden_state(encoder_out)  # (batch_size, decoder_dim)\n",
        "\n",
        "        # We won't decode at the <end> position, since we've finished generating as soon as we generate <end>\n",
        "        # So, decoding lengths are actual lengths - 1\n",
        "        decode_lengths = (caption_lengths - 1).tolist()\n",
        "\n",
        "        # Create tensors to hold word predicion scores and alphas\n",
        "        predictions = torch.zeros(batch_size, max(decode_lengths), vocab_size).to(device)\n",
        "        alphas = torch.zeros(batch_size, max(decode_lengths), num_pixels).to(device)\n",
        "\n",
        "        # At each time-step, decode by\n",
        "        # attention-weighing the encoder's output based on the decoder's previous hidden state output\n",
        "        # then generate a new word in the decoder with the previous word and the attention weighted encoding\n",
        "        for t in range(max(decode_lengths)):\n",
        "            batch_size_t = sum([l > t for l in decode_lengths])\n",
        "            attention_weighted_encoding, alpha = self.attention(encoder_out[:batch_size_t],\n",
        "                                                                h[:batch_size_t])\n",
        "            gate = self.sigmoid(self.f_beta(h[:batch_size_t]))  # gating scalar, (batch_size_t, encoder_dim)\n",
        "            attention_weighted_encoding = gate * attention_weighted_encoding\n",
        "            h, c = self.decode_step(\n",
        "                torch.cat([embeddings[:batch_size_t, t, :], attention_weighted_encoding], dim=1),\n",
        "                (h[:batch_size_t], c[:batch_size_t]))  # (batch_size_t, decoder_dim)\n",
        "            preds = self.fc(self.dropout(h))  # (batch_size_t, vocab_size)\n",
        "            predictions[:batch_size_t, t, :] = preds\n",
        "            alphas[:batch_size_t, t, :] = alpha\n",
        "\n",
        "        return predictions, encoded_captions, decode_lengths, alphas, sort_ind"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gddi9-OWLkqT"
      },
      "source": [
        "# Pytoch dataset transformation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GEXi427NFbPk"
      },
      "source": [
        "# h = h5py.File('train_images.hdf5', 'r')\n",
        "# h['images'].shape[0]\n",
        "# a = h.get('images')\n",
        "# h['images']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egEdtSr267C7"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import h5py\n",
        "import json\n",
        "import os\n",
        "\n",
        "\n",
        "class CaptionDataset(Dataset):\n",
        "    def __init__(self, split):\n",
        "        \"\"\"\n",
        "        :split, one of 'train', 'val'\n",
        "        \"\"\"\n",
        "        self.split = split\n",
        "\n",
        "        # Open hdf5 file where images are stored\n",
        "        self.h = h5py.File(os.path.join(self.split + '_images.hdf5'), 'r')\n",
        "        self.imgs = self.h.get('images')\n",
        "\n",
        "        # Captions per image\n",
        "        self.cpi = self.h.attrs['captions_per_image']\n",
        "\n",
        "        # Load encoded captions \n",
        "        with open(os.path.join(self.split + '_captions.json'), 'r') as j:\n",
        "          self.captions = json.load(j)\n",
        "\n",
        "        # Load caption lengths\n",
        "        with open(os.path.join(self.split + '_caption_length.json'), 'r') as j:\n",
        "          self.caplens = json.load(j)\n",
        "        \n",
        "        # Total number of datapoints\n",
        "        self.dataset_size = self.imgs.shape[0]\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        # Remember, the Nth caption corresponds to the (N // captions_per_image)th image\n",
        "        img = torch.FloatTensor(self.imgs[i//self.cpi] / 255.)\n",
        "        normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        img = transforms.Compose([normalize])(img)\n",
        "\n",
        "        caption = torch.LongTensor(self.captions[i])\n",
        "        caplen = torch.LongTensor([self.caplens[i]])\n",
        "\n",
        "        if self.split is 'train':\n",
        "            return img, caption, caplen\n",
        "        else:\n",
        "            # For validation of testing, also return all 'captions_per_image' captions to find BLEU-4 score\n",
        "            all_captions = torch.LongTensor(\n",
        "                self.captions[((i // self.cpi) * self.cpi):(((i // self.cpi) * self.cpi) + self.cpi)])\n",
        "            return img, caption, caplen, all_captions\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.dataset_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fh54sUaU5aUk"
      },
      "source": [
        "# Model training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCdA_a6PJBE2"
      },
      "source": [
        "##Initialize parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZTWV83qq6dy"
      },
      "source": [
        "import time\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim\n",
        "import torch.utils.data\n",
        "import torchvision.transforms as transforms\n",
        "from torch import nn\n",
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "# Model hyper-parameters\n",
        "emb_dim = 512  # dimension of word embeddings\n",
        "attention_dim = 512  # dimension of attention linear layers\n",
        "decoder_dim = 512  # dimension of decoder RNN\n",
        "dropout = 0.5\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  \n",
        "cudnn.benchmark = True  \n",
        "\n",
        "epochs = 30  \n",
        "batch_size = 32\n",
        "decoder_lr = 4e-4  \n",
        "alpha_c = 1.  # regularization parameter for 'doubly stochastic attention'\n",
        "best_bleu4 = 0.  # BLEU-4 score right now"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YcsBgmB6H6C6"
      },
      "source": [
        "## Training per epoch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRS2xtHsH5nu"
      },
      "source": [
        "def train(train_loader, encoder, decoder, loss_function, decoder_optimizer, epoch):\n",
        "    \"\"\"\n",
        "    Performs one epoch's training.\n",
        "    :param train_loader: DataLoader for training data\n",
        "    :param encoder: encoder model\n",
        "    :param decoder: decoder model\n",
        "    :param loss_function: loss layer\n",
        "    :param decoder_optimizer: optimizer to update decoder's weights\n",
        "    :param epoch: epoch number\n",
        "    \"\"\"\n",
        "\n",
        "    decoder.train()  # train mode (dropout and batchnorm is used)\n",
        "    encoder.train()\n",
        "\n",
        "    losses = AverageMeter()  # loss (per word decoded)\n",
        "    top5accs = AverageMeter()  # top5 accuracy\n",
        "\n",
        "    # Load by batches\n",
        "    for i, (imgs, caps, caplens) in enumerate(train_loader):\n",
        "        # Remember to use GPU\n",
        "        imgs = imgs.to(device)\n",
        "        caps = caps.to(device)\n",
        "        caplens = caplens.to(device)\n",
        "\n",
        "        # Encoding\n",
        "        encoded_imgs = encoder.forward(imgs)\n",
        "        # Decoding\n",
        "        preds, caps_sorted, decode_lengths, alphas, sort_ind = decoder.forward(encoded_imgs,caps,caplens)\n",
        "\n",
        "        # Since we decoded starting with <start>, the targets are all words after <start>, up to <end>\n",
        "        targets = caps_sorted[:, 1:]\n",
        "\n",
        "        # Remove timesteps that we didn't decode at, or are pads\n",
        "        preds, _, _, _ = pack_padded_sequence(preds, decode_lengths, batch_first=True)\n",
        "        targets, _, _, _ = pack_padded_sequence(targets, decode_lengths, batch_first=True)\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = loss_function(preds, targets)\n",
        "\n",
        "        # Add doubly stochastic attention regularization\n",
        "        loss += alpha_c * ((1. - alphas.sum(dim=1)) ** 2).mean()\n",
        "\n",
        "        # Back prop on decoder only\n",
        "        loss.backward()\n",
        "      \n",
        "        # Update weights\n",
        "        decoder_optimizer.step()\n",
        "        decoder_optimizer.zero_grad()\n",
        "\n",
        "        # Keep track of metrics\n",
        "        losses.update(loss.item(), sum(decode_lengths))\n",
        "        top5accs.update(accuracy(preds, targets, 5), sum(decode_lengths))\n",
        "\n",
        "        # Print status\n",
        "        if i % 100 == 0:\n",
        "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
        "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
        "                  'Top-5 Accuracy {top5.val:.3f} ({top5.avg:.3f})'.format(epoch, i, len(train_loader),\n",
        "                                                                          loss=losses,\n",
        "                                                                          top5=top5accs))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWgvfAFLH_ov"
      },
      "source": [
        "## Validation per epoch *"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUWGxZZ3H_z_"
      },
      "source": [
        "def validate(val_loader, encoder, decoder, loss_function):\n",
        "    \"\"\"\n",
        "    Performs one epoch's validation.\n",
        "    :param val_loader: DataLoader for validation data.\n",
        "    :param encoder: encoder model\n",
        "    :param decoder: decoder model\n",
        "    :param loss_function: loss layer\n",
        "    :return: BLEU-4 score\n",
        "    \"\"\"\n",
        "    # eval mode (no dropout or batchnorm)\n",
        "    decoder.eval()  \n",
        "    encoder.eval()\n",
        "\n",
        "    losses = AverageMeter()\n",
        "    top5accs = AverageMeter()\n",
        "\n",
        "    references = list()  # references (true captions) for calculating BLEU-4 score\n",
        "    hypotheses = list()  # hypotheses (predictions)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Batches\n",
        "        for i, (imgs, caps, caplens, allcaps) in enumerate(val_loader):\n",
        "            imgs = imgs.to(device)\n",
        "            caps = caps.to(device)\n",
        "            caplens = caplens.to(device)\n",
        "\n",
        "            # Forward prop.\n",
        "            encoded_imgs = encoder(imgs)\n",
        "            scores, caps_sorted, decode_lengths, alphas, sort_ind = decoder(encoded_imgs, caps, caplens)\n",
        "            targets = caps_sorted[:, 1:]\n",
        "\n",
        "            # Remove timesteps that we didn't decode at, or are pads\n",
        "            # pack_padded_sequence is an easy trick to do this\n",
        "            scores_copy = scores.clone()\n",
        "\n",
        "            scores, _, _, _ = pack_padded_sequence(scores, decode_lengths, batch_first=True)\n",
        "            targets, _, _, _ = pack_padded_sequence(targets, decode_lengths, batch_first=True)\n",
        "\n",
        "            # Calculate loss\n",
        "            loss = loss_function(scores, targets)\n",
        "\n",
        "            # Add doubly stochastic attention regularization\n",
        "            loss += alpha_c * ((1. - alphas.sum(dim=1)) ** 2).mean()\n",
        "\n",
        "            # Keep track of metrics\n",
        "            losses.update(loss.item(), sum(decode_lengths))\n",
        "            top5 = accuracy(scores, targets, 5)\n",
        "            top5accs.update(top5, sum(decode_lengths))\n",
        "\n",
        "            if i % 100 == 0:\n",
        "                print('Validation: [{0}/{1}]\\t'\n",
        "                      'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
        "                      'Top-5 Accuracy {top5.val:.3f} ({top5.avg:.3f})\\t'.format(i, len(val_loader),\n",
        "                                                                                loss=losses, top5=top5accs))\n",
        "\n",
        "            # Store references (true captions), and hypothesis (prediction) for each image\n",
        "            # If for n images, we have n hypotheses, and references a, b, c... for each image, we need -\n",
        "            # references = [[ref1a, ref1b, ref1c], [ref2a, ref2b], ...], hypotheses = [hyp1, hyp2, ...]\n",
        "\n",
        "            # References\n",
        "            allcaps = allcaps[sort_ind]  # because images were sorted in the decoder\n",
        "            for j in range(allcaps.shape[0]):\n",
        "                img_caps = allcaps[j].tolist()\n",
        "                img_captions = list(\n",
        "                    map(lambda c: [w for w in c if w not in {word_map['<start>'], word_map['<pad>']}],\n",
        "                        img_caps))  # remove <start> and pads\n",
        "                references.append(img_captions)\n",
        "\n",
        "            # Hypotheses\n",
        "            _, preds = torch.max(scores_copy, dim=2)\n",
        "            preds = preds.tolist()\n",
        "            temp_preds = list()\n",
        "            for j, p in enumerate(preds):\n",
        "                temp_preds.append(preds[j][:decode_lengths[j]])  # remove pads\n",
        "            preds = temp_preds\n",
        "            hypotheses.extend(preds)\n",
        "\n",
        "            assert len(references) == len(hypotheses)\n",
        "\n",
        "        # Calculate BLEU-4 scores\n",
        "        bleu4 = corpus_bleu(references, hypotheses)\n",
        "\n",
        "        print(\n",
        "            '\\n * LOSS - {loss.avg:.3f}, TOP-5 ACCURACY - {top5.avg:.3f}, BLEU-4 - {bleu}\\n'.format(\n",
        "                loss=losses,\n",
        "                top5=top5accs,\n",
        "                bleu=bleu4))\n",
        "\n",
        "    return bleu4\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMz3PcBUIGQ6"
      },
      "source": [
        "## Start training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehs0beBuq_mM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ca48a373-0e05-4a40-99e7-f3182d18fec6"
      },
      "source": [
        "# Read word map\n",
        "word_map_file = 'word_map.json'\n",
        "with open(word_map_file, 'r') as j:\n",
        "    word_map = json.load(j)\n",
        "\n",
        "# Initialize encoder, and we don't train it\n",
        "encoder = Encoder()\n",
        "encoder_optimizer = None\n",
        "\n",
        "# Initialize decoder, and adam optimizer\n",
        "decoder = DecoderWithAttention(attention_dim, emb_dim, decoder_dim, len(word_map))\n",
        "decoder_optimizer = torch.optim.Adam(\n",
        "    params=filter(lambda p: p.requires_grad, decoder.parameters()),\n",
        "    lr=decoder_lr\n",
        ")\n",
        "\n",
        "\n",
        "# Move to GPU, if available\n",
        "decoder = decoder.to(device)\n",
        "encoder = encoder.to(device)\n",
        "\n",
        "# Loss function\n",
        "loss_function = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "# Custom dataloaders\n",
        "train_loader = torch.utils.data.DataLoader(CaptionDataset('train'), batch_size=batch_size, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(CaptionDataset('val'), batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Epochs\n",
        "for epoch in range(epochs):\n",
        "    # One epoch's training\n",
        "    train(train_loader, encoder, decoder, loss_function, decoder_optimizer, epoch)\n",
        "\n",
        "    # One epoch's validation\n",
        "    recent_bleu4 = validate(val_loader=val_loader,\n",
        "                            encoder=encoder,\n",
        "                            decoder=decoder,\n",
        "                            loss_function=loss_function)\n",
        "\n",
        "    # Save checkpoint\n",
        "    is_best = recent_bleu4 > best_bleu4\n",
        "    best_bleu4 = max(recent_bleu4, best_bleu4)\n",
        "\n",
        "    save_checkpoint('coco', epoch, 0, encoder, decoder, encoder_optimizer,\n",
        "                    decoder_optimizer, recent_bleu4, is_best)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: [0][0/26]\tLoss 11.1749 (11.1749)\tTop-5 Accuracy 0.000 (0.000)\n",
            "Validation: [0/26]\tLoss 6.4707 (6.4707)\tTop-5 Accuracy 38.440 (38.440)\t\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " * LOSS - 6.670, TOP-5 ACCURACY - 34.740, BLEU-4 - 0.11608515502843306\n",
            "\n",
            "Epoch: [1][0/26]\tLoss 6.9640 (6.9640)\tTop-5 Accuracy 35.440 (35.440)\n",
            "Validation: [0/26]\tLoss 5.9584 (5.9584)\tTop-5 Accuracy 39.481 (39.481)\t\n",
            "\n",
            " * LOSS - 6.013, TOP-5 ACCURACY - 37.653, BLEU-4 - 0.27738807069616994\n",
            "\n",
            "Epoch: [2][0/26]\tLoss 6.0708 (6.0708)\tTop-5 Accuracy 34.286 (34.286)\n",
            "Validation: [0/26]\tLoss 5.6096 (5.6096)\tTop-5 Accuracy 41.551 (41.551)\t\n",
            "\n",
            " * LOSS - 5.787, TOP-5 ACCURACY - 38.754, BLEU-4 - 0.29736372450441007\n",
            "\n",
            "Epoch: [3][0/26]\tLoss 5.9926 (5.9926)\tTop-5 Accuracy 35.429 (35.429)\n",
            "Validation: [0/26]\tLoss 5.5444 (5.5444)\tTop-5 Accuracy 39.892 (39.892)\t\n",
            "\n",
            " * LOSS - 5.569, TOP-5 ACCURACY - 39.780, BLEU-4 - 0.01818392190427821\n",
            "\n",
            "Epoch: [4][0/26]\tLoss 6.0335 (6.0335)\tTop-5 Accuracy 36.188 (36.188)\n",
            "Validation: [0/26]\tLoss 5.5172 (5.5172)\tTop-5 Accuracy 41.111 (41.111)\t\n",
            "\n",
            " * LOSS - 5.382, TOP-5 ACCURACY - 42.027, BLEU-4 - 0.0176615378819742\n",
            "\n",
            "Epoch: [5][0/26]\tLoss 5.5514 (5.5514)\tTop-5 Accuracy 41.644 (41.644)\n",
            "Validation: [0/26]\tLoss 5.0896 (5.0896)\tTop-5 Accuracy 45.714 (45.714)\t\n",
            "\n",
            " * LOSS - 5.148, TOP-5 ACCURACY - 45.517, BLEU-4 - 0.03974808172018723\n",
            "\n",
            "Epoch: [6][0/26]\tLoss 5.3453 (5.3453)\tTop-5 Accuracy 41.243 (41.243)\n",
            "Validation: [0/26]\tLoss 4.9093 (4.9093)\tTop-5 Accuracy 50.392 (50.392)\t\n",
            "\n",
            " * LOSS - 4.960, TOP-5 ACCURACY - 47.928, BLEU-4 - 0.05667449784894198\n",
            "\n",
            "Epoch: [7][0/26]\tLoss 5.1636 (5.1636)\tTop-5 Accuracy 44.324 (44.324)\n",
            "Validation: [0/26]\tLoss 4.5992 (4.5992)\tTop-5 Accuracy 53.994 (53.994)\t\n",
            "\n",
            " * LOSS - 4.768, TOP-5 ACCURACY - 51.047, BLEU-4 - 0.0849557290167242\n",
            "\n",
            "Epoch: [8][0/26]\tLoss 4.9087 (4.9087)\tTop-5 Accuracy 50.769 (50.769)\n",
            "Validation: [0/26]\tLoss 4.6443 (4.6443)\tTop-5 Accuracy 53.659 (53.659)\t\n",
            "\n",
            " * LOSS - 4.576, TOP-5 ACCURACY - 54.996, BLEU-4 - 0.10889558966998805\n",
            "\n",
            "Epoch: [9][0/26]\tLoss 4.7401 (4.7401)\tTop-5 Accuracy 52.738 (52.738)\n",
            "Validation: [0/26]\tLoss 4.1460 (4.1460)\tTop-5 Accuracy 60.000 (60.000)\t\n",
            "\n",
            " * LOSS - 4.369, TOP-5 ACCURACY - 58.093, BLEU-4 - 0.12783396219761223\n",
            "\n",
            "Epoch: [10][0/26]\tLoss 4.1980 (4.1980)\tTop-5 Accuracy 61.062 (61.062)\n",
            "Validation: [0/26]\tLoss 4.1905 (4.1905)\tTop-5 Accuracy 58.671 (58.671)\t\n",
            "\n",
            " * LOSS - 4.206, TOP-5 ACCURACY - 60.449, BLEU-4 - 0.15208126587382345\n",
            "\n",
            "Epoch: [11][0/26]\tLoss 4.5626 (4.5626)\tTop-5 Accuracy 54.155 (54.155)\n",
            "Validation: [0/26]\tLoss 4.3392 (4.3392)\tTop-5 Accuracy 60.614 (60.614)\t\n",
            "\n",
            " * LOSS - 4.079, TOP-5 ACCURACY - 63.034, BLEU-4 - 0.17349776799663016\n",
            "\n",
            "Epoch: [12][0/26]\tLoss 4.2984 (4.2984)\tTop-5 Accuracy 60.267 (60.267)\n",
            "Validation: [0/26]\tLoss 3.9557 (3.9557)\tTop-5 Accuracy 63.712 (63.712)\t\n",
            "\n",
            " * LOSS - 3.955, TOP-5 ACCURACY - 64.114, BLEU-4 - 0.16812622001817903\n",
            "\n",
            "Epoch: [13][0/26]\tLoss 3.9840 (3.9840)\tTop-5 Accuracy 64.407 (64.407)\n",
            "Validation: [0/26]\tLoss 3.8758 (3.8758)\tTop-5 Accuracy 64.136 (64.136)\t\n",
            "\n",
            " * LOSS - 3.836, TOP-5 ACCURACY - 65.510, BLEU-4 - 0.18478633525515575\n",
            "\n",
            "Epoch: [14][0/26]\tLoss 4.2069 (4.2069)\tTop-5 Accuracy 60.174 (60.174)\n",
            "Validation: [0/26]\tLoss 3.9289 (3.9289)\tTop-5 Accuracy 65.450 (65.450)\t\n",
            "\n",
            " * LOSS - 3.726, TOP-5 ACCURACY - 67.910, BLEU-4 - 0.20001909191823947\n",
            "\n",
            "Epoch: [15][0/26]\tLoss 3.9939 (3.9939)\tTop-5 Accuracy 62.712 (62.712)\n",
            "Validation: [0/26]\tLoss 3.7390 (3.7390)\tTop-5 Accuracy 65.676 (65.676)\t\n",
            "\n",
            " * LOSS - 3.629, TOP-5 ACCURACY - 68.717, BLEU-4 - 0.2210373146198682\n",
            "\n",
            "Epoch: [16][0/26]\tLoss 3.7392 (3.7392)\tTop-5 Accuracy 66.393 (66.393)\n",
            "Validation: [0/26]\tLoss 3.6789 (3.6789)\tTop-5 Accuracy 66.398 (66.398)\t\n",
            "\n",
            " * LOSS - 3.518, TOP-5 ACCURACY - 70.212, BLEU-4 - 0.22214210733575201\n",
            "\n",
            "Epoch: [17][0/26]\tLoss 3.9548 (3.9548)\tTop-5 Accuracy 64.571 (64.571)\n",
            "Validation: [0/26]\tLoss 3.3768 (3.3768)\tTop-5 Accuracy 72.191 (72.191)\t\n",
            "\n",
            " * LOSS - 3.411, TOP-5 ACCURACY - 72.349, BLEU-4 - 0.22452343040531086\n",
            "\n",
            "Epoch: [18][0/26]\tLoss 3.7411 (3.7411)\tTop-5 Accuracy 66.667 (66.667)\n",
            "Validation: [0/26]\tLoss 3.3606 (3.3606)\tTop-5 Accuracy 74.022 (74.022)\t\n",
            "\n",
            " * LOSS - 3.352, TOP-5 ACCURACY - 73.451, BLEU-4 - 0.23753908735557688\n",
            "\n",
            "Epoch: [19][0/26]\tLoss 3.6658 (3.6658)\tTop-5 Accuracy 68.715 (68.715)\n",
            "Validation: [0/26]\tLoss 3.2826 (3.2826)\tTop-5 Accuracy 75.833 (75.833)\t\n",
            "\n",
            " * LOSS - 3.240, TOP-5 ACCURACY - 74.847, BLEU-4 - 0.24354798287074475\n",
            "\n",
            "Epoch: [20][0/26]\tLoss 3.3658 (3.3658)\tTop-5 Accuracy 74.114 (74.114)\n",
            "Validation: [0/26]\tLoss 3.2285 (3.2285)\tTop-5 Accuracy 75.543 (75.543)\t\n",
            "\n",
            " * LOSS - 3.165, TOP-5 ACCURACY - 76.287, BLEU-4 - 0.24678106218174412\n",
            "\n",
            "Epoch: [21][0/26]\tLoss 3.3976 (3.3976)\tTop-5 Accuracy 75.691 (75.691)\n",
            "Validation: [0/26]\tLoss 3.0382 (3.0382)\tTop-5 Accuracy 76.177 (76.177)\t\n",
            "\n",
            " * LOSS - 3.111, TOP-5 ACCURACY - 76.483, BLEU-4 - 0.2556665423381722\n",
            "\n",
            "Epoch: [22][0/26]\tLoss 3.4188 (3.4188)\tTop-5 Accuracy 71.579 (71.579)\n",
            "Validation: [0/26]\tLoss 3.0995 (3.0995)\tTop-5 Accuracy 74.648 (74.648)\t\n",
            "\n",
            " * LOSS - 3.049, TOP-5 ACCURACY - 77.869, BLEU-4 - 0.27161595772031266\n",
            "\n",
            "Epoch: [23][0/26]\tLoss 3.4065 (3.4065)\tTop-5 Accuracy 73.164 (73.164)\n",
            "Validation: [0/26]\tLoss 2.8444 (2.8444)\tTop-5 Accuracy 82.836 (82.836)\t\n",
            "\n",
            " * LOSS - 2.994, TOP-5 ACCURACY - 79.254, BLEU-4 - 0.2774252209572084\n",
            "\n",
            "Epoch: [24][0/26]\tLoss 3.2273 (3.2273)\tTop-5 Accuracy 75.462 (75.462)\n",
            "Validation: [0/26]\tLoss 3.0002 (3.0002)\tTop-5 Accuracy 79.948 (79.948)\t\n",
            "\n",
            " * LOSS - 2.935, TOP-5 ACCURACY - 79.887, BLEU-4 - 0.27962076553081566\n",
            "\n",
            "Epoch: [25][0/26]\tLoss 3.1082 (3.1082)\tTop-5 Accuracy 76.163 (76.163)\n",
            "Validation: [0/26]\tLoss 2.9933 (2.9933)\tTop-5 Accuracy 78.393 (78.393)\t\n",
            "\n",
            " * LOSS - 2.889, TOP-5 ACCURACY - 80.530, BLEU-4 - 0.2935406039684126\n",
            "\n",
            "Epoch: [26][0/26]\tLoss 3.2606 (3.2606)\tTop-5 Accuracy 71.274 (71.274)\n",
            "Validation: [0/26]\tLoss 2.8352 (2.8352)\tTop-5 Accuracy 84.746 (84.746)\t\n",
            "\n",
            " * LOSS - 2.824, TOP-5 ACCURACY - 82.744, BLEU-4 - 0.3083640283654316\n",
            "\n",
            "Epoch: [27][0/26]\tLoss 3.1393 (3.1393)\tTop-5 Accuracy 76.344 (76.344)\n",
            "Validation: [0/26]\tLoss 2.7408 (2.7408)\tTop-5 Accuracy 81.471 (81.471)\t\n",
            "\n",
            " * LOSS - 2.770, TOP-5 ACCURACY - 83.879, BLEU-4 - 0.3100122187687769\n",
            "\n",
            "Epoch: [28][0/26]\tLoss 3.0456 (3.0456)\tTop-5 Accuracy 80.481 (80.481)\n",
            "Validation: [0/26]\tLoss 2.6669 (2.6669)\tTop-5 Accuracy 87.193 (87.193)\t\n",
            "\n",
            " * LOSS - 2.710, TOP-5 ACCURACY - 85.166, BLEU-4 - 0.31030890318368326\n",
            "\n",
            "Epoch: [29][0/26]\tLoss 2.9719 (2.9719)\tTop-5 Accuracy 82.385 (82.385)\n",
            "Validation: [0/26]\tLoss 2.7021 (2.7021)\tTop-5 Accuracy 87.187 (87.187)\t\n",
            "\n",
            " * LOSS - 2.642, TOP-5 ACCURACY - 86.409, BLEU-4 - 0.3195476624496363\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3C5wGoKK_k3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}